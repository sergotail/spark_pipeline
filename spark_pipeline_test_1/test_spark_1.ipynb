{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import common packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import errno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import findspark and import&init spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--master yarn                                      --deploy-mode cluster                                      --conf \"spark.yarn.am.port\"=8025                                     pyspark-shell\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/ser/Dev/Spark/spark-2.1.0-bin-hadoop2.7\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/home/ser/Dev/Python/2/anaconda2/bin/python\"\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master local[4] pyspark-shell\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master yarn \\\n",
    "                                     --deploy-mode cluster \\\n",
    "                                     --conf \\\"spark.yarn.am.port\\\"=8025\\\n",
    "                                     pyspark-shell\"\n",
    "print os.environ['PYSPARK_SUBMIT_ARGS']\n",
    "os.environ['HADOOP_CONF_DIR'] = \"/home/ser/Dev/Hadoop/hadoop-2.7.3/etc/hadoop\"\n",
    "findspark.init()\n",
    "\n",
    "#import numpy as np\n",
    "#import sep\n",
    "#from operator import add\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import task-oriented packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import astromatic_wrapper as aw\n",
    "from astropy.io import fits\n",
    "#import pydoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants (in future go to config file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exec_mode = 'test'\n",
    "wrk_path = '/home/ser/Dev/Notebooks/spark_pipeline_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AstroConfigurer:\n",
    "    \n",
    "    def __init__(self, check_dirs_existance=False, **kwargs):\n",
    "        \n",
    "        # declare class attributes and first init\n",
    "        self._exec_mode = ''\n",
    "        self._wrk_path = None\n",
    "        self._cur_path = None\n",
    "        #self._cur_timestamp = None\n",
    "        self._logs_timestamp = None\n",
    "        self._attr_prefix = '_'\n",
    "        self._dirname_delim = '_'\n",
    "        self._paths = None\n",
    "        self._log_paths = []\n",
    "        self._check_dirs_existance = check_dirs_existance\n",
    "        \n",
    "        # check not None kwargs\n",
    "        for kwarg in kwargs:\n",
    "            if kwargs[kwarg] is not None:\n",
    "                # check if str attrs are actually str typed\n",
    "                if kwarg.endswith('_mode') \\\n",
    "                or kwarg.endswith('_path') \\\n",
    "                or kwarg.endswith('_timestamp') \\\n",
    "                or kwarg.endswith('_prefix') \\\n",
    "                or kwarg.endswith('_delim'):\n",
    "                    if not isinstance(kwargs[kwarg], str):\n",
    "                        raise TypeError(kwarg + ' argument has str type, but '\n",
    "                                       + type(kwargs[kwarg]).__name__ + ' typed value specified')\n",
    "                # check if path attrs are valid pathnames\n",
    "                if kwarg.endswith('_path') and not self._is_pathname_valid(kwargs[kwarg]):\n",
    "                    raise ValueError('value ' + kwargs[kwarg] + ' for argument ' + kwarg\n",
    "                                         + ' is not a valid pathname')\n",
    "        \n",
    "        # init class attrs if it set directly\n",
    "        for kwarg in kwargs:\n",
    "            if hasattr(self, self._attr_prefix + kwarg) and kwargs[kwarg] is not None:\n",
    "                setattr(self, self._attr_prefix + kwarg, kwargs[kwarg])\n",
    "                \n",
    "        # init strictly valued attrs\n",
    "        self._cur_path = os.getcwd()\n",
    "        #self._update_cur_timestamp()\n",
    "        self._update_logs_timestamp()\n",
    "        \n",
    "        # build attributes\n",
    "        if not self._wrk_path:\n",
    "            self._wrk_path = self._cur_path\n",
    "        else:\n",
    "            self._wrk_path = self._build_path(self._cur_path, self._wrk_path)\n",
    "            \n",
    "        # set paths to work dirs\n",
    "        self._paths = {\n",
    "            'temp': os.path.join(self._wrk_path, 'temp'),\n",
    "            'logs': os.path.join(self._wrk_path, 'logs'),\n",
    "            'config': os.path.join(self._wrk_path, 'config'),\n",
    "            'catalogs': os.path.join(self._wrk_path, 'catalogs'),\n",
    "            'stacks': os.path.join(self._wrk_path, 'stacks'),\n",
    "            'images': os.path.join(self._wrk_path, 'images')\n",
    "                }\n",
    "        \n",
    "        # set paths kwargs if specified\n",
    "        for kwarg in kwargs:\n",
    "            paths_dir = kwarg.rstrip('_path')\n",
    "            if paths_dir in self._paths:\n",
    "                self._paths[paths_dir] = self._build_path(self._wrk_path, kwargs[kwarg])\n",
    "                \n",
    "        # create dirs tree for application according to paths\n",
    "        for path in self._paths:\n",
    "            self._create_dir(self._paths[path], False, self._check_dirs_existance)\n",
    "\n",
    "        # check if paths are ok\n",
    "        for val in self._paths.values():\n",
    "            print val\n",
    "        \n",
    "    def _update_logs_timestamp(self):\n",
    "        self._logs_timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    \n",
    "    def _build_dir_name(self, *args, **kwargs):\n",
    "        delim = '_'\n",
    "        if 'delim' in kwargs:\n",
    "            delim = kwargs['delim']\n",
    "        res = ''\n",
    "        for arg in args:\n",
    "            if arg:\n",
    "                res += arg + delim\n",
    "        return res.rstrip(delim)\n",
    "    \n",
    "    def _is_pathname_valid(self, pathname):\n",
    "        try:\n",
    "            if not isinstance(pathname, str) or not pathname:\n",
    "                return False\n",
    "\n",
    "            _, pathname = os.path.splitdrive(pathname)\n",
    "            root_dirname = os.environ.get('HOMEDRIVE', 'C:') if sys.platform == 'win32' else os.path.sep\n",
    "            assert os.path.isdir(root_dirname)\n",
    "            root_dirname = root_dirname.rstrip(os.path.sep) + os.path.sep\n",
    "\n",
    "            for pathname_part in pathname.split(os.path.sep):\n",
    "                try:\n",
    "                    os.lstat(root_dirname + pathname_part)\n",
    "                except OSError as os_err:\n",
    "                    if hasattr(os_err, 'winerror'):\n",
    "                        if os_err.winerror == ERROR_INVALID_NAME:\n",
    "                            return False\n",
    "                    elif os_err.errno in {errno.ENAMETOOLONG, errno.ERANGE}:\n",
    "                        return False\n",
    "        except TypeError:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def _is_dir_writeable(self, pathname, base=None):\n",
    "        dirname = os.path.dirname(pathname) or (base if base else self._wrk_path)\n",
    "        return os.access(dirname, os.W_OK)\n",
    "    \n",
    "    def _is_dir_exists(self, pathname, check_pathname=True):\n",
    "        try:\n",
    "            return (True if not check_pathname else self._is_pathname_valid(pathname)) \\\n",
    "                    and os.path.isdir(pathname)\n",
    "        except OSError:\n",
    "            return False\n",
    "    \n",
    "    def _create_dir(self, pathname, check_pathname=True, check_exist=False):\n",
    "        try:\n",
    "            if check_pathname:\n",
    "                if not self._is_pathname_valid(pathname):\n",
    "                    raise ValueError('cannot create dir: pathname is not valid')\n",
    "            os.makedirs(pathname)\n",
    "        except OSError as os_err:\n",
    "            if check_exist and os_err.errno == errno.EEXIST or os_err.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "    def _build_path(self, base, pathname):\n",
    "        return pathname if os.path.isabs(pathname) else os.path.join(base, pathname)\n",
    "    \n",
    "    def get_exec_mode(self):\n",
    "        return self._exec_mode\n",
    "    \n",
    "    def set_exec_mode(self, new_exec_mode):\n",
    "        if isinstance(new_exec_mode, str):\n",
    "            self._exec_mode = new_exec_mode\n",
    "        else:\n",
    "            raise ValueError('Execution mode must have str type, not ' + type(new_exec_mode).__name__)\n",
    "        return self._exec_mode\n",
    "    \n",
    "    def get_paths(self):\n",
    "        return self._paths\n",
    "    \n",
    "    def get_paths_dir_names(self):\n",
    "        return [dir_name for dir_name in self._paths]\n",
    "    \n",
    "    def new_log_dir(self, check_exist=True):\n",
    "        self._update_logs_timestamp()\n",
    "        self._log_paths.append(os.path.join(self._paths['logs'], \n",
    "                                            self._build_dir_name(self._exec_mode, 'log', \n",
    "                                                                 self._logs_timestamp, \n",
    "                                                                 delim=self._dirname_delim)))\n",
    "        self._create_dir(self._log_paths[-1], False, check_exist)\n",
    "        return self._log_paths[-1]\n",
    "    \n",
    "    def get_last_log_dir(self):\n",
    "        return self._log_paths[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ser/Dev/Notebooks/spark_pipeline_1/catalogs\n",
      "/home/ser/Dev/Notebooks/spark_pipeline_1/logs\n",
      "/home/ser/Dev/Notebooks/spark_pipeline_1/temp\n",
      "/home/ser/Dev/Notebooks/spark_pipeline_1/images\n",
      "/home/ser/Dev/Notebooks/spark_pipeline_1/config\n",
      "/home/ser/Dev/Notebooks/spark_pipeline_1/stacks\n",
      "/home/ser/Dev/Notebooks/spark_pipeline_1/logs/log_2017-04-05_06-20-54\n",
      "['catalogs', 'logs', 'temp', 'images', 'config', 'stacks']\n"
     ]
    }
   ],
   "source": [
    "cfg = AstroConfigurer(False, wrk_path=wrk_path)\n",
    "cfg.new_log_dir()\n",
    "print cfg.get_last_log_dir()\n",
    "print cfg.get_paths_dir_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark code separately yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_1(input_item):\n",
    "    file_name = input_item[0].lstrip('file:')\n",
    "    catalog_name = os.path.join(cfg.get_paths()['catalogs'], os.path.basename(file_name.replace('.fit', '.cat')))\n",
    "    sex_kwargs_1 = {'code': 'SExtractor'}\n",
    "    sex_kwargs_1['config_file'] = os.path.join(cfg.get_paths()['config'], 'default.sex')\n",
    "    sex_kwargs_1['config'] = {'CATALOG_NAME': catalog_name}\n",
    "    sex_kwargs_1['config']['CATALOG_TYPE'] = 'FITS_LDAC'\n",
    "    sex_kwargs_1['config']['FILTER'] = 'N'\n",
    "    sex_kwargs_1['temp_path'] = cfg.get_paths()['temp']\n",
    "    sex_kwargs_1['params'] = ['NUMBER', 'EXT_NUMBER', 'XWIN_IMAGE', 'YWIN_IMAGE', 'AWIN_IMAGE', 'BWIN_IMAGE',\n",
    "                              'ERRAWIN_IMAGE','ERRBWIN_IMAGE', 'ERRTHETAWIN_IMAGE', 'ERRA_WORLD', 'ERRB_WORLD', \n",
    "                              'ERRTHETA_WORLD', 'X_WORLD', 'Y_WORLD', 'XWIN_WORLD', 'YWIN_WORLD', \n",
    "                              'FLUX_AUTO', 'FLUX_MAX', 'MAG_AUTO', 'FLUXERR_AUTO', \n",
    "                              'FLAGS', 'FLUX_RADIUS', 'ELONGATION']\n",
    "    sextractor = aw.api.Astromatic(**sex_kwargs_1)\n",
    "    sextractor.run(file_name)\n",
    "    #with open(catalog_name, \"r\") as catalog:\n",
    "        #output_item = ('file:' + catalog_name, catalog)\n",
    "    return catalog_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_2(input_item):\n",
    "    file_name = input_item[0]\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_3(input_item):\n",
    "    file_name = os.path.basename(input_item[0])\n",
    "    tmp_file_name = os.path.join(cfg.get_paths()['temp'], file_name)\n",
    "    catalog_name = os.path.join(cfg.get_paths()['catalogs'], file_name.replace('.fit', '.cat'))\n",
    "    with open(tmp_file_name, \"w+\") as fits_file:\n",
    "        fits_file.write(input_item[1])\n",
    "    sex_kwargs_1 = {'code': 'SExtractor'}\n",
    "    sex_kwargs_1['config_file'] = os.path.join(cfg.get_paths()['config'], 'default.sex')\n",
    "    sex_kwargs_1['config'] = {'CATALOG_NAME': catalog_name}\n",
    "    sex_kwargs_1['config']['CATALOG_TYPE'] = 'FITS_LDAC'\n",
    "    sex_kwargs_1['config']['FILTER'] = 'N'\n",
    "    sex_kwargs_1['temp_path'] = cfg.get_paths()['temp']\n",
    "    sex_kwargs_1['params'] = ['NUMBER', 'EXT_NUMBER', 'XWIN_IMAGE', 'YWIN_IMAGE', 'AWIN_IMAGE', 'BWIN_IMAGE',\n",
    "                              'ERRAWIN_IMAGE','ERRBWIN_IMAGE', 'ERRTHETAWIN_IMAGE', 'ERRA_WORLD', 'ERRB_WORLD', \n",
    "                              'ERRTHETA_WORLD', 'X_WORLD', 'Y_WORLD', 'XWIN_WORLD', 'YWIN_WORLD', \n",
    "                              'FLUX_AUTO', 'FLUX_MAX', 'MAG_AUTO', 'FLUXERR_AUTO', \n",
    "                              'FLAGS', 'FLUX_RADIUS', 'ELONGATION']\n",
    "    sextractor = aw.api.Astromatic(**sex_kwargs_1)\n",
    "    sextractor.run(tmp_file_name)\n",
    "    with open(catalog_name, \"r\") as catalog:\n",
    "        output_item = ('file:' + catalog_name, catalog)\n",
    "    return output_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending the driver its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7ca98ebf83f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SourceExtractor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#files = sc.binaryFiles(cfg.get_paths()['images'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinaryFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://localhost:9000/ser/images/images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ser/Dev/Spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/home/ser/Dev/Spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ser/Dev/Spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/java_gateway.pyc\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mcallback_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgateway_port\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending the driver its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# In Windows, ensure the Java child processes do not linger after Python has exited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending the driver its port number"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext(appName=\"SourceExtractor\")\n",
    "    #files = sc.binaryFiles(cfg.get_paths()['images'])\n",
    "    files = sc.binaryFiles(\"hdfs://localhost:9000/ser/images/images\")\n",
    "    print files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = files.filter(lambda x: x[0].endswith('.fit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print files.map(extract).reduce(join_cats)\n",
    "#files.map(extract_1).map(lambda x: [x]).reduce(lambda a, b: a + b)\n",
    "catalogs = files.map(extract_3).collect()\n",
    "#len(catalogs)\n",
    "#catalogs.map(lambda x: [x]).reduce(lambda a, b: a + b)\n",
    "#catalogs.saveAsSequenceFile(cfg.get_paths()['catalogs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'file:/home/ser/Dev/Notebooks/spark_pipeline_1/catalogs/GRB130427_R60_001_001.cat',\n",
       " <closed file '<uninitialized file>', mode '<uninitialized file>' at 0x7f573ba3e6f0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
