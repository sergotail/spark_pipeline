{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import common packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import errno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import findspark and import&init spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--master yarn                                      --deploy-mode cluster                                      --conf \"spark.yarn.am.port\"=8025                                     pyspark-shell\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/ser/Dev/Spark/spark-2.1.0-bin-hadoop2.7\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/home/ser/Dev/Python/2/anaconda2/bin/python\"\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master local[4] pyspark-shell\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master yarn \\\n",
    "                                     --deploy-mode cluster \\\n",
    "                                     --conf \\\"spark.yarn.am.port\\\"=8025\\\n",
    "                                     pyspark-shell\"\n",
    "print os.environ['PYSPARK_SUBMIT_ARGS']\n",
    "os.environ['HADOOP_CONF_DIR'] = \"/home/ser/Dev/Hadoop/hadoop-2.7.3/etc/hadoop\"\n",
    "findspark.init()\n",
    "\n",
    "#import numpy as np\n",
    "#import sep\n",
    "#from operator import add\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import task-oriented packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import astromatic_wrapper as aw\n",
    "from astropy.io import fits\n",
    "#import pydoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants (in future go to config file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exec_mode = 'test'\n",
    "wrk_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['c', 'b'])\n"
     ]
    }
   ],
   "source": [
    "a = {'a', 'b'}\n",
    "b = {'a', 'c'}\n",
    "print a ^ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AEDirsTreeConfigurer:\n",
    "    \n",
    "    def __init__(self, check_dirs_existance=False, **kwargs):\n",
    "        \n",
    "        # declare non-setable class attributes\n",
    "        self._class_non_setable_attrs = ['_attr_prefix', '_cur_path', '_paths_names']\n",
    "        self._cur_path = os.getcwd()\n",
    "        self._attr_prefix = '_'\n",
    "        self._paths_names = ['catalogs', 'logs', 'temp', 'images', 'config', 'stacks']\n",
    "        \n",
    "        # declare class attributes and first init\n",
    "        self._exec_mode = ''\n",
    "        self._wrk_path = None\n",
    "        #self._cur_timestamp = None\n",
    "        self._logs_timestamp = None\n",
    "        self._dirname_delim = '_'\n",
    "        self._paths = None\n",
    "        self._log_paths = []\n",
    "        \n",
    "        # init strictly valued attrs (may be reinitialized later in this __init__)\n",
    "        #self._update_cur_timestamp()\n",
    "        self._update_logs_timestamp()\n",
    "        \n",
    "        # check check_dirs_existance argument type and set it if ok\n",
    "        if isinstance(check_dirs_existance, bool):\n",
    "            self._check_dirs_existance = check_dirs_existance\n",
    "        else:\n",
    "            raise TypeError('check_dirs_existance argument must have bool type, but ' \n",
    "                                       + type(check_dirs_existance).__name__ + ' typed value specified')\n",
    "        \n",
    "        # check if kwargs contains only supported args\n",
    "        class_setable_attrs = {attr for attr in dir(self) if not callable(getattr(self, attr)) and \n",
    "                       not attr.startswith('__')} - set(self._class_non_setable_attrs)\n",
    "        class_setable_attrs = {attr.lstrip(self._attr_prefix) for attr in class_setable_attrs}\n",
    "        paths_attrs = {'%s_path' % name for name in self._paths_names}\n",
    "        kwargs_setable_attrs = class_setable_attrs | paths_attrs\n",
    "        kwargs_attrs = {kwarg for kwarg in kwargs}\n",
    "        err_attrs = (kwargs_setable_attrs | kwargs_attrs) - kwargs_setable_attrs\n",
    "        if err_attrs:\n",
    "            raise AttributeError('argument' + ('s ' if len(err_attrs) > 1 else ' ') + ', '.join(err_attrs) \n",
    "                                 + ' passed, but no attributes exist for it')\n",
    "        \n",
    "        # check not None kwargs\n",
    "        for kwarg in kwargs:\n",
    "            if kwargs[kwarg] is not None:\n",
    "                # check if str args are actually str typed\n",
    "                if kwarg.endswith('_mode') \\\n",
    "                or kwarg.endswith('_path') \\\n",
    "                or kwarg.endswith('_timestamp') \\\n",
    "                or kwarg.endswith('_delim'):\n",
    "                    if not isinstance(kwargs[kwarg], str):\n",
    "                        raise TypeError(kwarg + ' argument must have str type, but '\n",
    "                                       + type(kwargs[kwarg]).__name__ + ' typed value specified')\n",
    "                # check if path args are valid pathnames\n",
    "                if kwarg.endswith('_path') and not self._is_pathname_valid(kwargs[kwarg]):\n",
    "                    raise ValueError('value \\\"' + kwargs[kwarg] + '\\\" for argument ' + kwarg\n",
    "                                         + ' is not a valid pathname')\n",
    "                # check paths kwarg, if it is\n",
    "                if kwarg == 'paths':\n",
    "                    # check paths, its keys and values types\n",
    "                    if not isinstance(kwargs[kwarg], dict):\n",
    "                        raise TypeError(kwarg + ' argument must have dict type, but '\n",
    "                                       + type(kwargs[kwarg]).__name__ + ' typed value specified')\n",
    "                    for k, v in kwargs[kwarg].iteritems():\n",
    "                        if not isinstance(k, str):\n",
    "                            raise TypeError('keys for paths dict must have str type, but '\n",
    "                                       + type(k).__name__ + ' typed value specified')\n",
    "                        if not isinstance(v, str):\n",
    "                            raise TypeError('values for paths dict must have str type, but '\n",
    "                                       + type(v).__name__ + ' typed value specified')\n",
    "                    # check paths keys\n",
    "                    paths_keys = set(kwargs[kwarg].keys())\n",
    "                    err_paths_keys = (set(self._paths_names) | paths_keys) - set(self._paths_names)\n",
    "                    if err_paths_keys:\n",
    "                        raise KeyError('key' + ('s ' if len(err_paths_keys) > 1 else ' ')\n",
    "                                       + ', '.join(err_paths_keys)\n",
    "                                       + (' are' if len(err_paths_keys) > 1 else ' is')\n",
    "                                       + ' wrong')\n",
    "                    # check paths values\n",
    "                    for k, v in kwargs[kwarg].iteritems():\n",
    "                        if not self._is_pathname_valid(v):\n",
    "                            raise ValueError('value ' + v + ' for key ' + k + ' is not a valid pathname')\n",
    "                    \n",
    "        \n",
    "        # init class attrs if it set directly in kwargs (paths dict will be initialized later)\n",
    "        for kwarg in kwargs:\n",
    "            if kwarg != 'paths' and hasattr(self, self._attr_prefix + kwarg) and kwargs[kwarg] is not None:\n",
    "                setattr(self, self._attr_prefix + kwarg, kwargs[kwarg])\n",
    "        \n",
    "        # build wrk_path\n",
    "        if not self._wrk_path:\n",
    "            self._wrk_path = self._cur_path\n",
    "        else:\n",
    "            self._wrk_path = self._build_path(self._cur_path, self._wrk_path)\n",
    "            \n",
    "        # set default paths to work dirs\n",
    "        self._paths = {\n",
    "            'temp': os.path.join(self._wrk_path, 'temp'),\n",
    "            'logs': os.path.join(self._wrk_path, 'logs'),\n",
    "            'config': os.path.join(self._wrk_path, 'config'),\n",
    "            'catalogs': os.path.join(self._wrk_path, 'catalogs'),\n",
    "            'stacks': os.path.join(self._wrk_path, 'stacks'),\n",
    "            'images': os.path.join(self._wrk_path, 'images')\n",
    "                }\n",
    "        \n",
    "        # set paths dict or its part from kwargs, if specified\n",
    "        if 'paths' in kwargs:\n",
    "            if kwargs['paths']:\n",
    "                for k, v in kwargs['paths'].iteritems():\n",
    "                    self._paths[k] = self._build_path(self._wrk_path, v)\n",
    "        \n",
    "        # set paths to work dirs from kwargs, if specified\n",
    "        for kwarg in kwargs.keys():\n",
    "            paths_dir = kwarg.replace('_path', '')\n",
    "            if paths_dir in self._paths:\n",
    "                pathname = kwargs[kwarg]\n",
    "                self._paths[paths_dir] = self._build_path(self._wrk_path, pathname)\n",
    "        \n",
    "    def _update_logs_timestamp(self):\n",
    "        self._logs_timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    \n",
    "    def _build_dir_name(self, *args, **kwargs):\n",
    "        delim = '_'\n",
    "        if 'delim' in kwargs:\n",
    "            delim = kwargs['delim']\n",
    "        res = ''\n",
    "        for arg in args:\n",
    "            if arg:\n",
    "                res += arg + delim\n",
    "        return res.rstrip(delim)\n",
    "    \n",
    "    def _is_pathname_valid(self, pathname):\n",
    "        try:\n",
    "            if not isinstance(pathname, str) or not pathname:\n",
    "                return False\n",
    "\n",
    "            _, pathname = os.path.splitdrive(pathname)\n",
    "            root_dirname = os.environ.get('HOMEDRIVE', 'C:') if sys.platform == 'win32' else os.path.sep\n",
    "            assert os.path.isdir(root_dirname)\n",
    "            root_dirname = root_dirname.rstrip(os.path.sep) + os.path.sep\n",
    "\n",
    "            for pathname_part in pathname.split(os.path.sep):\n",
    "                try:\n",
    "                    os.lstat(root_dirname + pathname_part)\n",
    "                except OSError as os_err:\n",
    "                    if hasattr(os_err, 'winerror'):\n",
    "                        if os_err.winerror == ERROR_INVALID_NAME:\n",
    "                            return False\n",
    "                    elif os_err.errno in {errno.ENAMETOOLONG, errno.ERANGE}:\n",
    "                        return False\n",
    "        except TypeError:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def _is_dir_writeable(self, pathname, base=None):\n",
    "        dirname = os.path.dirname(pathname) or (base if base else self._wrk_path)\n",
    "        return os.access(dirname, os.W_OK)\n",
    "    \n",
    "    def _is_dir_exists(self, pathname, check_pathname=True):\n",
    "        try:\n",
    "            return (True if not check_pathname else self._is_pathname_valid(pathname)) \\\n",
    "                    and os.path.isdir(pathname)\n",
    "        except OSError:\n",
    "            return False\n",
    "    \n",
    "    def _create_dir(self, pathname, check_pathname=True, check_exist=False):\n",
    "        try:\n",
    "            if check_pathname:\n",
    "                if not self._is_pathname_valid(pathname):\n",
    "                    raise ValueError('cannot create dir: pathname is not valid')\n",
    "            os.makedirs(pathname)\n",
    "        except OSError as os_err:\n",
    "            if check_exist and os_err.errno == errno.EEXIST or os_err.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "    def _build_path(self, base, pathname):\n",
    "        return pathname if os.path.isabs(pathname) else os.path.join(base, pathname)\n",
    "    \n",
    "    def build_dirs_tree(self):\n",
    "        # create dirs tree for application according to paths dict\n",
    "        for path in self._paths:\n",
    "            self._create_dir(self._paths[path], True, self._check_dirs_existance)\n",
    "    \n",
    "    def get_exec_mode(self):\n",
    "        return self._exec_mode\n",
    "    \n",
    "    def set_exec_mode(self, new_exec_mode):\n",
    "        if isinstance(new_exec_mode, str):\n",
    "            self._exec_mode = new_exec_mode\n",
    "        else:\n",
    "            raise TypeError('exec_mode attribute has str type, but passed argument has type ' + \\\n",
    "                             type(new_exec_mode).__name__)\n",
    "        return self._exec_mode\n",
    "    \n",
    "    def get_paths(self):\n",
    "        return self._paths\n",
    "    \n",
    "    def get_paths_dir_names(self):\n",
    "        return self._paths_names\n",
    "    \n",
    "    def new_log_dir(self, check_exist=True):\n",
    "        self._update_logs_timestamp()\n",
    "        self._log_paths.append(os.path.join(self._paths['logs'], \n",
    "                                            self._build_dir_name(self._exec_mode, 'log', \n",
    "                                                                 self._logs_timestamp, \n",
    "                                                                 delim=self._dirname_delim)))\n",
    "        self._create_dir(self._log_paths[-1], False, check_exist)\n",
    "        return self._log_paths[-1]\n",
    "    \n",
    "    def get_last_log_dir(self):\n",
    "        return self._log_paths[-1]\n",
    "    \n",
    "    def get_dir(self, dirname):\n",
    "        return self._paths[dirname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'catalogs': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/catalogs',\n",
       " 'config': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/config',\n",
       " 'images': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/images',\n",
       " 'logs': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/logs',\n",
       " 'stacks': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/stacks',\n",
       " 'temp': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/temp'}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = AEDirsTreeConfigurer(False, images_path='images')\n",
    "cfg.build_dirs_tree()\n",
    "cfg.new_log_dir()\n",
    "cfg.get_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AEConfig:\n",
    "    def __init__(self):\n",
    "        self._scamp_required_sex_params = ['NUMBER', 'EXT_NUMBER', 'XWIN_IMAGE', 'YWIN_IMAGE', \n",
    "                                           'AWIN_IMAGE', 'BWIN_IMAGE', 'ERRAWIN_IMAGE','ERRBWIN_IMAGE', \n",
    "                                           'ERRTHETAWIN_IMAGE', 'ERRA_WORLD', 'ERRB_WORLD','ERRTHETA_WORLD', \n",
    "                                           'X_WORLD', 'Y_WORLD', 'XWIN_WORLD', 'YWIN_WORLD', 'FLUX_AUTO', \n",
    "                                           'FLUX_MAX', 'MAG_AUTO', 'FLUXERR_AUTO', \n",
    "                                           # flags not supported yet (need Weightwatcher)\n",
    "                                           #'IMAFLAGS_ISO', 'FLAGS_WEIGHT',\n",
    "                                           'FLAGS', 'FLUX_RADIUS', 'ELONGATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['catalogs_path',\n",
       " 'logs_path',\n",
       " 'temp_path',\n",
       " 'images_path',\n",
       " 'config_path',\n",
       " 'stacks_path']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark code separately yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_1(input_item):\n",
    "    file_name = input_item[0].replace('file:', '')\n",
    "    catalog_name = os.path.join(cfg.get_paths()['catalogs'], os.path.basename(file_name.replace('.fit', '.cat')))\n",
    "    sex_kwargs_1 = {'code': 'SExtractor'}\n",
    "    sex_kwargs_1['config_file'] = os.path.join(cfg.get_paths()['config'], 'default.sex')\n",
    "    sex_kwargs_1['config'] = {'CATALOG_NAME': catalog_name}\n",
    "    sex_kwargs_1['config']['CATALOG_TYPE'] = 'FITS_LDAC'\n",
    "    sex_kwargs_1['config']['FILTER'] = 'N'\n",
    "    sex_kwargs_1['temp_path'] = cfg.get_paths()['temp']\n",
    "    sex_kwargs_1['params'] = ['NUMBER', 'EXT_NUMBER', 'XWIN_IMAGE', 'YWIN_IMAGE', 'AWIN_IMAGE', 'BWIN_IMAGE',\n",
    "                              'ERRAWIN_IMAGE','ERRBWIN_IMAGE', 'ERRTHETAWIN_IMAGE', 'ERRA_WORLD', 'ERRB_WORLD', \n",
    "                              'ERRTHETA_WORLD', 'X_WORLD', 'Y_WORLD', 'XWIN_WORLD', 'YWIN_WORLD', \n",
    "                              'FLUX_AUTO', 'FLUX_MAX', 'MAG_AUTO', 'FLUXERR_AUTO', \n",
    "                              'FLAGS', 'FLUX_RADIUS', 'ELONGATION']\n",
    "    sextractor = aw.api.Astromatic(**sex_kwargs_1)\n",
    "    sextractor.run(file_name)\n",
    "    #with open(catalog_name, \"r\") as catalog:\n",
    "        #output_item = ('file:' + catalog_name, catalog)\n",
    "    return catalog_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_2(input_item):\n",
    "    file_name = input_item[0]\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_3(input_item):\n",
    "    file_name = os.path.basename(input_item[0])\n",
    "    tmp_file_name = os.path.join(cfg.get_paths()['temp'], file_name)\n",
    "    catalog_name = os.path.join(cfg.get_paths()['catalogs'], file_name.replace('.fit', '.cat'))\n",
    "    with open(tmp_file_name, \"w+\") as fits_file:\n",
    "        fits_file.write(input_item[1])\n",
    "    sex_kwargs_1 = {'code': 'SExtractor'}\n",
    "    sex_kwargs_1['config_file'] = os.path.join(cfg.get_paths()['config'], 'default.sex')\n",
    "    sex_kwargs_1['config'] = {'CATALOG_NAME': catalog_name}\n",
    "    sex_kwargs_1['config']['CATALOG_TYPE'] = 'FITS_LDAC'\n",
    "    sex_kwargs_1['config']['FILTER'] = 'N'\n",
    "    sex_kwargs_1['temp_path'] = cfg.get_paths()['temp']\n",
    "    sex_kwargs_1['params'] = ['NUMBER', 'EXT_NUMBER', 'XWIN_IMAGE', 'YWIN_IMAGE', 'AWIN_IMAGE', 'BWIN_IMAGE',\n",
    "                              'ERRAWIN_IMAGE','ERRBWIN_IMAGE', 'ERRTHETAWIN_IMAGE', 'ERRA_WORLD', 'ERRB_WORLD', \n",
    "                              'ERRTHETA_WORLD', 'X_WORLD', 'Y_WORLD', 'XWIN_WORLD', 'YWIN_WORLD', \n",
    "                              'FLUX_AUTO', 'FLUX_MAX', 'MAG_AUTO', 'FLUXERR_AUTO', \n",
    "                              'FLAGS', 'FLUX_RADIUS', 'ELONGATION']\n",
    "    sextractor = aw.api.Astromatic(**sex_kwargs_1)\n",
    "    sextractor.run(tmp_file_name)\n",
    "    with open(catalog_name, \"r\") as catalog:\n",
    "        output_item = ('file:' + catalog_name, catalog)\n",
    "    return output_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending the driver its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7ca98ebf83f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SourceExtractor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#files = sc.binaryFiles(cfg.get_paths()['images'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinaryFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://localhost:9000/ser/images/images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ser/Dev/Spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/home/ser/Dev/Spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ser/Dev/Spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/java_gateway.pyc\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mcallback_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgateway_port\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending the driver its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# In Windows, ensure the Java child processes do not linger after Python has exited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending the driver its port number"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext(appName=\"SourceExtractor\")\n",
    "    #files = sc.binaryFiles(cfg.get_paths()['images'])\n",
    "    files = sc.binaryFiles(\"hdfs://localhost:9000/ser/images/images\")\n",
    "    print files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = files.filter(lambda x: x[0].endswith('.fit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print files.map(extract).reduce(join_cats)\n",
    "#files.map(extract_1).map(lambda x: [x]).reduce(lambda a, b: a + b)\n",
    "catalogs = files.map(extract_3).collect()\n",
    "#len(catalogs)\n",
    "#catalogs.map(lambda x: [x]).reduce(lambda a, b: a + b)\n",
    "#catalogs.saveAsSequenceFile(cfg.get_paths()['catalogs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'file:/home/ser/Dev/Notebooks/spark_pipeline_1/catalogs/GRB130427_R60_001_001.cat',\n",
       " <closed file '<uninitialized file>', mode '<uninitialized file>' at 0x7f573ba3e6f0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
