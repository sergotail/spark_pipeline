{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import common packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import copy\n",
    "import shutil\n",
    "from operator import itemgetter\n",
    "from functools import partial\n",
    "from itertools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import pyspark and other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkFiles, SparkConf\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from py4j.java_gateway import JavaGateway\n",
    "#import pydoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import task-oriented packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import astromatic_wrapper as aw #TODO: install aw to itaf virtual env if needed\n",
    "from astropy.io import fits\n",
    "import pyraf\n",
    "from pyraf import iraf\n",
    "sys.path.insert(0, '/home/ser/Dev/astro_engine/code')\n",
    "from astro_utils import AEDirsTreeConfigurer, AEJsonConfigLoader\n",
    "from fits_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Used functions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Some help functions</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_to_file(list_, filename, sep='\\n'):\n",
    "    with open(filename, 'w') as fid:\n",
    "        for item in list_:\n",
    "            fid.write('%s%s' % (str(item), sep))\n",
    "\n",
    "def to_list(something):\n",
    "    return something if isinstance(something, list) else [something]\n",
    "\n",
    "def fits_ImageHDU_from_file(filename, mode='denywrite', lazy_load_hdus=False):\n",
    "    f = fits.open(filename, mode='denywrite', lazy_load_hdus=False)\n",
    "    fits_image = fits.ImageHDU(data=f[0].data, header=f[0].header)\n",
    "    f.close()\n",
    "    return fits_image\n",
    "\n",
    "def filter_fits_by_header(fits_filenames, fits_path=None, match_any=False, ignore_case=True,\n",
    "                          ignore_edge_spaces=True, hdu_num=0, **kwargs):\n",
    "    iffs = [os.path.join(fits_path, os.path.basename(iff)) if fits_path is not None else iff\\\n",
    "            for iff in to_list(fits_filenames)]\n",
    "    res = []\n",
    "    for iff in iffs:\n",
    "        fits_file_header = fits.getheader(iff, hdu_num)\n",
    "        keywords_match = 0\n",
    "        for keyword, value in kwargs.iteritems():\n",
    "            if keyword in fits_file_header:\n",
    "                header_val = str(fits_file_header[keyword]).strip() if ignore_edge_spaces \\\n",
    "                else str(fits_file_header[keyword])\n",
    "                header_val = header_val.lower() if ignore_case else header_val\n",
    "                kwargs_val = str(value).strip() if ignore_edge_spaces else str(value)\n",
    "                kwargs_val = kwargs_val.lower() if ignore_case else kwargs_val\n",
    "                if header_val == kwargs_val:\n",
    "                    keywords_match += 1\n",
    "                    if match_any:\n",
    "                        break\n",
    "        if (keywords_match and match_any) or (keywords_match == len(kwargs)):\n",
    "            res.append(iff)\n",
    "    return res\n",
    "\n",
    "def header_keywords_values(fits_filenames, fits_path=None, keywords=['SIMPLE'], keys_keywords=True, lower_case=True,\n",
    "                           split_edge_spaces=True, hdu_num=0):\n",
    "    iffs = [os.path.join(fits_path, os.path.basename(iff)) if fits_path is not None else iff \\\n",
    "            for iff in to_list(fits_filenames)]\n",
    "    kws = to_list(keywords)\n",
    "    res = {}\n",
    "    for iff in iffs:\n",
    "        fits_file_header = fits.getheader(iff, hdu_num)\n",
    "        if keys_keywords:\n",
    "            for keyword in kws:\n",
    "                if keyword not in res.keys():\n",
    "                    res[keyword] = {}\n",
    "                if keyword in fits_file_header:\n",
    "                    val = fits_file_header[keyword]\n",
    "                    if val not in res[keyword].keys():\n",
    "                        res[keyword][val] = []\n",
    "                    res[keyword][val].append(iff)\n",
    "        else:   \n",
    "            for keyword in kws:\n",
    "                if keyword in fits_file_header:\n",
    "                    res[iff] = {keyword: fits_file_header[keyword]}\n",
    "                else:\n",
    "                    res[iff] = {keyword: None}\n",
    "    return res\n",
    "\n",
    "def change_header(fits_filename, fits_path=None, new_path=None, clobber=True, **kwargs):\n",
    "    iff = os.path.join(fits_path, os.path.basename(fits_filename)) if fits_path is not None else fits_filename\n",
    "    data, header = fits.getdata(iff, header=True)\n",
    "    for keyword, value in kwargs.iteritems():\n",
    "        header[keyword] = value\n",
    "    if new_path is None:\n",
    "        new_path = fits_path\n",
    "    iff = os.path.join(new_path, os.path.basename(iff))\n",
    "    fits.writeto(iff, data, header, clobber=clobber)\n",
    "    return iff\n",
    "\n",
    "def to_header_str_format(str_value, length=8):\n",
    "    return str_value.strip().ljust(length)\n",
    "\n",
    "def apply_to_fits(fits_filenames, fits_path=None, func=None):\n",
    "    iffs = [os.path.join(fits_path, os.path.basename(iff)) if fits_path is not None else iff \\\n",
    "            for iff in to_list(fits_filenames)]\n",
    "    if not func:\n",
    "        return iffs\n",
    "    res = []\n",
    "    for iff in iffs:\n",
    "        res.append(func(iff))\n",
    "    return res\n",
    "\n",
    "def images_stat(images, fields='mode,mean', return_dict=True, stdout=1, stderr=2):\n",
    "    iraf.images(_doprint=0)\n",
    "    if return_dict:\n",
    "        iraf.images.imstat.setParam('format', 'no')\n",
    "        #iraf.images.imstat.setParam('')\n",
    "    iraf.images.imstat.setParam('fields', fields)\n",
    "    images = images if isinstance(images, list) else [images]\n",
    "    res = []\n",
    "    for image in images:\n",
    "        res.append(iraf.imstat(image, Stdout=stdout, Stderr=stderr))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Basic reduction</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_superbias(input_images_names, paths, pipeline_config, pipeline_stage, pipeline_step):\n",
    "    # TODO: check config, stage and step for needed for this function keys\n",
    "    # select pipeline stage and step in config\n",
    "    step_config = copy.deepcopy(pipeline_config['pipeline_stages'][pipeline_stage]['stage_steps'][pipeline_step])\n",
    "    step_params_config = step_config['step_params']\n",
    "    software_params = step_params_config['software_params']\n",
    "    \n",
    "    input_tech_frames_dir = paths[step_params_config['input_params']['tech_frames']['frames_dir']]\n",
    "    output_tech_frames_dir = paths[step_params_config['output_params']['tech_frames']['frames_dir']]\n",
    "\n",
    "    input_bias_config = step_params_config['input_params']['tech_frames']['bias']\n",
    "    input_bias_config['format'] = to_ext_format(input_bias_config['format'])\n",
    "    output_bias_config = step_params_config['output_params']['tech_frames']['bias']\n",
    "    output_bias_config['format'] = to_ext_format(output_bias_config['format'])\n",
    "    \n",
    "    input_bias_dir = os.path.join(input_tech_frames_dir, input_bias_config['sub_dir'])\n",
    "    output_bias_dir = os.path.join(output_tech_frames_dir, output_bias_config['sub_dir'])\n",
    "\n",
    "    superbias_filename = os.path.join(output_bias_dir,\n",
    "                                      (output_bias_config['name'] + output_bias_config['format']))\n",
    "    \n",
    "    filter_bias_kwargs = {input_bias_config['type_keyword']: input_bias_config['type_keyword_value']}\n",
    "    return filter_bias_kwargs\n",
    "    input_bias_filenames = filter_fits_by_header(input_images_names, input_bias_dir, \n",
    "                                                 **filter_bias_kwargs)\n",
    "    \n",
    "    iraf.noao.imred(_doprint=0)\n",
    "    iraf.noao.imred.ccdred(_doprint=0)\n",
    "    iraf.noao.imred.ccdred.instrument = os.path.join(paths[software_params['config']['INSTRUMENT_DIR']],\n",
    "                                                     software_params['config']['INSTRUMENT_FILE'])\n",
    "    \n",
    "    if 'ccdtype' in input_bias_config.keys():\n",
    "        iraf.noao.imred.ccdred.zerocombine.setParam('ccdtype', input_bias_config['ccdtype'])\n",
    "    else:\n",
    "        iraf.noao.imred.ccdred.zerocombine.setParam('ccdtype', '')\n",
    "    if 'gain_keyword' in input_bias_config.keys():\n",
    "        gain = fits.getheader(input_bias_filenames[0], 0)[input_bias_config['gain_keyword']]\n",
    "        iraf.noao.imred.ccdred.zerocombine.setParam('gain', gain)\n",
    "    \n",
    "    bias_list = os.path.join(paths['temp'], 'bias_list.txt')\n",
    "    list_to_file(input_bias_filenames, bias_list)\n",
    "    \n",
    "    iraf.noao.imred.ccdred.zerocombine(input='@' + bias_list, output=superbias_filename, \n",
    "                                       process='no', delete='no', clobber='no')\n",
    "    \n",
    "    return superbias_filename\n",
    "\n",
    "def compute_superdark(input_images_names, paths, pipeline_config, pipeline_stage, pipeline_step):\n",
    "    # TODO: check config, stage and step for needed for this function keys\n",
    "    # select pipeline stage and step in config\n",
    "    step_config = copy.deepcopy(pipeline_config['pipeline_stages'][pipeline_stage]['stage_steps'][pipeline_step])\n",
    "    step_params_config = step_config['step_params']\n",
    "    software_params = step_params_config['software_params']\n",
    "    \n",
    "    input_tech_frames_dir = paths[step_params_config['input_params']['tech_frames']['frames_dir']]\n",
    "    output_tech_frames_dir = paths[step_params_config['output_params']['tech_frames']['frames_dir']]\n",
    "\n",
    "    input_dark_config = step_params_config['input_params']['tech_frames']['bias']\n",
    "    input_dark_config['format'] = to_ext_format(input_dark_config['format'])\n",
    "    output_dark_config = step_params_config['output_params']['tech_frames']['bias']\n",
    "    output_dark_config['format'] = to_ext_format(output_dark_config['format'])\n",
    "    \n",
    "    input_dark_dir = os.path.join(input_tech_frames_dir, input_dark_config['sub_dir'])\n",
    "    output_dark_dir = os.path.join(output_tech_frames_dir, output_dark_config['sub_dir'])\n",
    "\n",
    "    superdark_filename = os.path.join(input_dark_dir,\n",
    "                                      (output_dark_config['name'] + output_dark_config['format']))\n",
    "    \n",
    "    filter_dark_kwargs = {input_dark_config['type_keyword']: input_dark_config['type_keyword_value']}\n",
    "    return filter_dark_kwargs\n",
    "    input_dark_filenames = filter_fits_by_header(input_images_names, input_dark_dir, \n",
    "                                                 **filter_dark_kwargs)\n",
    "    \n",
    "    iraf.noao.imred(_doprint=0)\n",
    "    iraf.noao.imred.ccdred(_doprint=0)\n",
    "    iraf.noao.imred.ccdred.instrument = os.path.join(paths[software_params['config']['INSTRUMENT_DIR']],\n",
    "                                                     software_params['config']['INSTRUMENT_FILE'])\n",
    "    \n",
    "    if 'ccdtype' in input_bias_config.keys():\n",
    "        iraf.noao.imred.ccdred.darkcombine.setParam('ccdtype', input_dark_config['ccdtype'])\n",
    "    else:\n",
    "        iraf.noao.imred.ccdred.darkcombine.setParam('ccdtype', '')\n",
    "    if 'gain_keyword' in input_bias_config.keys():\n",
    "        gain = fits.getheader(input_bias_filenames[0], 0)[input_dark_config['gain_keyword']]\n",
    "        iraf.noao.imred.ccdred.darkcombine.setParam('gain', gain)\n",
    "    \n",
    "    dark_list = os.path.join(paths['temp'], 'dark_list.txt')\n",
    "    list_to_file(input_dark_filenames, dark_list)\n",
    "    \n",
    "    iraf.noao.imred.ccdred.darkcombine(input='@' + dark_list, output=superdark_filename, \n",
    "                                       process='no', delete='no', clobber='no')\n",
    "    \n",
    "    return superdark_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_supers(input_images_names, paths, pipeline_config, pipeline_stage, pipeline_step):\n",
    "    # TODO: check config, stage and step for needed for this function keys\n",
    "    # select pipeline stage and step in config\n",
    "    step_config = copy.deepcopy(pipeline_config['pipeline_stages'][pipeline_stage]['stage_steps'][pipeline_step])\n",
    "    step_params_config = step_config['step_params']\n",
    "    input_params_config = step_params_config['input_params']\n",
    "    output_params_config = step_params_config['output_params']\n",
    "    \n",
    "    # prepare io params \n",
    "    exp_times = to_list(input_params_config['images_exp_times_secs'])\n",
    "    filters = to_list(input_params_config['images_filters'])\n",
    "    images_filter_keyword = input_params_config['images_filter_keyword']\n",
    "    input_tech_frames_dir = paths[input_params_config['tech_frames']['frames_dir']]\n",
    "    output_tech_frames_dir = paths[output_params_config['tech_frames']['frames_dir']]\n",
    "    use_bias = 'bias' in input_params_config['tech_frames']['use_frames']\n",
    "    use_dark = 'dark' in input_params_config['tech_frames']['use_frames']\n",
    "    use_flat = 'flat' in input_params_config['tech_frames']['use_frames']\n",
    "\n",
    "    \n",
    "    if use_bias:\n",
    "        superbias_filename = compute_superbias(input_images_names, paths, \n",
    "                                               pipeline_config, pipeline_stage, pipeline_step)   \n",
    "    if use_dark:\n",
    "        # compute superdarks for all exp_times for data images\n",
    "        #if no exp_time, take exp_time with biggest list of corresponding darks\n",
    "        exp_times_darks = {}\n",
    "        for exp_time in exp_times:\n",
    "            exp_times_darks[exp_time] = compute_superdark(input_images_names, paths, \n",
    "                                               pipeline_config, pipeline_stage, pipeline_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Catalog extraction basic function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_catalogs(input_images_names, paths, pipeline_config, pipeline_stage, pipeline_step):\n",
    "    # TODO: check config, stage and step for needed for this function keys\n",
    "    # select pipeline stage and step in config\n",
    "    step_config = copy.deepcopy(pipeline_config['pipeline_stages'][pipeline_stage]['stage_steps'][pipeline_step])\n",
    "    step_params_config = step_config['step_params']\n",
    "    \n",
    "    # use paths to fill path-depended params TODO: preprocess config to do this\n",
    "    step_params_config['software_params']['config_file'] = os.path.join(paths['config'],\n",
    "                                                                step_params_config['software_params']['config_file'])\n",
    "    step_params_config['software_params']['temp_path'] = paths[step_params_config['software_params']['temp_path']]\n",
    "    step_params_config['software_params']['config']['FILTER_NAME'] = os.path.join(paths['config'],\n",
    "                                                      step_params_config['software_params']['config']['FILTER_NAME'])\n",
    "    step_params_config['software_params']['config']['STARNNW_NAME'] = os.path.join(paths['config'],\n",
    "                                                      step_params_config['software_params']['config']['STARNNW_NAME'])\n",
    "    \n",
    "    # prepare io params   \n",
    "    # input images\n",
    "    input_images_format = to_ext_format(step_params_config['input_params']['images_format'])\n",
    "    input_images_dir = paths[step_params_config['input_params']['images_dir']]\n",
    "    iffs = build_filenames_list(input_images_names, new_ext=input_images_format, \n",
    "                                new_path=input_images_dir)\n",
    "    \n",
    "    # input psf models, if exist\n",
    "    look_for_psf = False\n",
    "    ipfs = range(len(iffs))\n",
    "    if 'psf_models_format' in step_params_config['input_params'].keys() and \\\n",
    "    'psf_models_dir' in step_params_config['input_params'].keys():\n",
    "        look_for_psf = True\n",
    "        input_psf_models_format = to_ext_format(step_params_config['input_params']['psf_models_format'])\n",
    "        input_psf_models_dir = paths[step_params_config['input_params']['psf_models_dir']]\n",
    "        ipfs = build_filenames_list(input_images_names, new_ext=input_psf_models_format, \n",
    "                                    new_path=input_psf_models_dir)\n",
    "    \n",
    "    # output catalogs\n",
    "    output_catalogs_format = to_ext_format(step_params_config['output_params']['catalogs_format'])\n",
    "    output_catalogs_dir = paths[step_params_config['output_params']['catalogs_dir']]\n",
    "    ocfs = build_filenames_list(input_images_names, new_ext=output_catalogs_format, \n",
    "                                    new_path=output_catalogs_dir)\n",
    "    \n",
    "    \n",
    "    if 'check_params_for_scamp_required' in step_params_config['output_params'].keys():\n",
    "        if step_params_config['output_params']['check_params_for_scamp_required'] == 'true':\n",
    "            missing_req_scamp_params = list(set(pipeline_config['scamp_required_catalog_params']) - \\\n",
    "            set(step_params_config['software_params']['params']))\n",
    "            step_params_config['software_params']['params'].extend(missing_req_scamp_params)\n",
    "    \n",
    "    if 'check_params_for_psfex_required' in step_params_config['output_params'].keys():\n",
    "        if step_params_config['output_params']['check_params_for_psfex_required'] == 'true':\n",
    "            missing_req_psfex_params = list(set(pipeline_config['psfex_required_catalog_params']) - \\\n",
    "            set(step_params_config['software_params']['params']))\n",
    "            step_params_config['software_params']['params'].extend(missing_req_psfex_params)\n",
    "    \n",
    "    # output is a list of catalogs full pathnames\n",
    "    catalogs = []\n",
    "    \n",
    "    for (iff, ipf, ocf) in zip(iffs, ipfs, ocfs):\n",
    "        software_params = step_params_config['software_params'] # TODO: deepcopy or not?\n",
    "        # TODO: is it needed or SExtractor recognizes it by itself?\n",
    "        if 'params_from_fits_header' in step_params_config['data_handle'].keys():\n",
    "            fits_file = fits.open(iff)\n",
    "            fits_file_header = fits_file[0].header\n",
    "            for param_name, fits_header_keyword in step_params_config['data_handle']['params_from_fits_header'].iteritems():\n",
    "                if fits_header_keyword in fits_file_header:\n",
    "                    software_params['config'][param_name] = str(fits_file[0].header[fits_header_keyword])\n",
    "        software_params['config']['CATALOG_NAME'] = ocf\n",
    "        if look_for_psf:\n",
    "            software_params['config']['PSF_NAME'] = ipf\n",
    "        sextractor = aw.api.Astromatic(**software_params)\n",
    "        #this_cmd, kwargs2 = sextractor.build_cmd(iff, **software_params)\n",
    "        #print this_cmd\n",
    "        sextractor.run(iff) #TODO: handle run status (maybe return it?)\n",
    "        catalogs.append(ocf)\n",
    "    \n",
    "    return catalogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Calculate calibration basic function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_calibration(input_images_names, paths, pipeline_config, pipeline_stage, pipeline_step):\n",
    "    # TODO: check config, stage and step for needed for this function keys\n",
    "    # select pipeline stage and step in config\n",
    "    step_config = copy.deepcopy(pipeline_config['pipeline_stages'][pipeline_stage]['stage_steps'][pipeline_step])\n",
    "    step_params_config = step_config['step_params']\n",
    "    \n",
    "    # use paths to fill path-depended params TODO: preprocess config to do this\n",
    "    step_params_config['software_params']['config_file'] = os.path.join(paths['config'],\n",
    "                                                                step_params_config['software_params']['config_file'])\n",
    "    step_params_config['software_params']['temp_path'] = paths[step_params_config['software_params']['temp_path']]\n",
    "    \n",
    "    # prepare io params   \n",
    "    # input catalogs\n",
    "    input_catalogs_format = to_ext_format(step_params_config['input_params']['catalogs_format'])\n",
    "    input_catalogs_dir = paths[step_params_config['input_params']['catalogs_dir']]\n",
    "    icfs = build_filenames_list(input_images_names, new_ext=input_catalogs_format, \n",
    "                                    new_path=input_catalogs_dir)\n",
    "    \n",
    "    # output is empty list\n",
    "    result = []\n",
    "    \n",
    "    calculate_calibration_separately = False\n",
    "    if 'calculate_calibration_separately' in step_params_config['output_params'].keys():\n",
    "        if step_params_config['output_params']['calculate_calibration_separately'] == 'true':\n",
    "            calculate_calibration_separately = True\n",
    "            \n",
    "    software_params = step_params_config['software_params'] # TODO: deepcopy or not?\n",
    "    \n",
    "    if calculate_calibration_separately:\n",
    "        for icf in icfs:\n",
    "            scamp = aw.api.Astromatic(**software_params)\n",
    "            #this_cmd, kwargs2 = scamp.build_cmd(icf, **software_params)\n",
    "            #print this_cmd\n",
    "            scamp.run(icf)\n",
    "    else:\n",
    "        scamp = aw.api.Astromatic(**software_params)\n",
    "        scamp.run(' '.join(icfs)) # TODO: create file with catalogs list, and pass as @catalogs_list\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Image SWarp basic function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def swarp_images(input_images_names, paths, pipeline_config, pipeline_stage, pipeline_step):\n",
    "    # TODO: check config, stage and step for needed for this function keys\n",
    "    # select pipeline stage and step in config\n",
    "    step_config = copy.deepcopy(pipeline_config['pipeline_stages'][pipeline_stage]['stage_steps'][pipeline_step])\n",
    "    step_params_config = step_config['step_params']\n",
    "    \n",
    "    # use paths to fill path-depended params TODO: preprocess config to do this\n",
    "    step_params_config['software_params']['config_file'] = os.path.join(paths['config'],\n",
    "                                                                step_params_config['software_params']['config_file'])\n",
    "    step_params_config['software_params']['temp_path'] = paths[step_params_config['software_params']['temp_path']]\n",
    "    step_params_config['software_params']['config']['RESAMPLE_DIR'] = paths[step_params_config['software_params']['config']['RESAMPLE_DIR']]\n",
    "    \n",
    "    # prepare io params\n",
    "    # input images\n",
    "    input_images_format = to_ext_format(step_params_config['input_params']['images_format'])\n",
    "    input_images_dir = paths[step_params_config['input_params']['images_dir']]\n",
    "    iffs = build_filenames_list(input_images_names, new_ext=input_images_format, \n",
    "                                new_path=input_images_dir)\n",
    "    \n",
    "    # output images\n",
    "    output_images_format = to_ext_format(step_params_config['output_params']['images_format'])\n",
    "    output_images_dir = paths[step_params_config['output_params']['images_dir']]\n",
    "    \n",
    "    # output weightmaps\n",
    "    output_weightmaps_format = to_ext_format(step_params_config['output_params']['weightmaps_format'])\n",
    "    output_weightmaps_dir = paths[step_params_config['output_params']['weightmaps_dir']]\n",
    "    \n",
    "    # output is a tuple of lists: swarped images full pathnames and weightmaps full pathnames\n",
    "    swarped_images, weightmaps = [], []\n",
    "    \n",
    "    swarp_separately = False\n",
    "    if 'swarp_separately' in step_params_config['output_params'].keys():\n",
    "        if step_params_config['output_params']['swarp_separately'] == 'true':\n",
    "            swarp_separately = True\n",
    "            \n",
    "    software_params = step_params_config['software_params'] # TODO: deepcopy or not?\n",
    "                   \n",
    "    if swarp_separately:\n",
    "        # prepare output_images_filenames and output_weightmaps_filenames (convert them to full pathname)\n",
    "        offs = build_filenames_list(input_images_names, new_ext=output_images_format, \n",
    "                                    new_path=output_images_dir)\n",
    "        owmfs = build_filenames_list(input_images_names, new_ext=output_weightmaps_format, \n",
    "                                    new_path=output_weightmaps_dir)\n",
    "        \n",
    "        for (iff, off, owmf) in zip(iffs, offs, owmfs):\n",
    "            software_params['config']['IMAGEOUT_NAME'] = off\n",
    "            software_params['config']['WEIGHTOUT_NAME'] = owmf\n",
    "            swarp = aw.api.Astromatic(**software_params)\n",
    "            swarp.run(iff)\n",
    "            swarped_images.append(off)\n",
    "            weightmaps.append(owmf)   \n",
    "    else:\n",
    "        # build coadded image and weightmap names\n",
    "        output_filename = pipeline_config['observation_id']\n",
    "        coadd_images_nums = sorted(\n",
    "            [os.path.basename(iff).rstrip(input_images_format).rsplit('_',1)[1] for iff in iffs])\n",
    "        nums_ranges = []\n",
    "        for k, g in groupby(enumerate(coadd_images_nums), lambda (i,x):int(i)-int(x)):\n",
    "            nums_range = map(itemgetter(1),g)\n",
    "            nums_ranges.append(nums_range[0] if len(nums_range) == 1 else nums_range[0] + '-' + nums_range[-1])\n",
    "        nums_ranges = '_'.join(nums_ranges)\n",
    "        output_image_name = output_filename + '_' + nums_ranges + output_images_format\n",
    "        output_image_name = os.path.join(output_images_dir, output_image_name)\n",
    "        output_weightmap_name = output_filename + '_' + nums_ranges + output_weightmaps_format\n",
    "        output_weightmap_name = os.path.join(output_weightmaps_dir, output_weightmap_name)\n",
    "        # run SWarp\n",
    "        software_params['config']['IMAGEOUT_NAME'] = output_image_name\n",
    "        software_params['config']['WEIGHTOUT_NAME'] = output_weightmap_name\n",
    "        swarp = aw.api.Astromatic(**software_params)\n",
    "        swarp.run(' '.join(iffs)) # TODO: create file with images list, and pass as @images_list\n",
    "        swarped_images.append(output_image_name)\n",
    "        weightmaps.append(output_weightmap_name)\n",
    "    \n",
    "    return swarped_images, weightmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Calculate PSF basic function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_psf(input_images_names, paths, pipeline_config, pipeline_stage, pipeline_step):\n",
    "    # TODO: check config, stage and step for needed for this function keys\n",
    "    # select pipeline stage and step in config\n",
    "    step_config = copy.deepcopy(pipeline_config['pipeline_stages'][pipeline_stage]['stage_steps'][pipeline_step])\n",
    "    step_params_config = step_config['step_params']\n",
    "    \n",
    "    # use paths to fill path-depended params TODO: preprocess config to do this\n",
    "    step_params_config['software_params']['config_file'] = os.path.join(paths['config'],\n",
    "                                                                step_params_config['software_params']['config_file'])\n",
    "    step_params_config['software_params']['temp_path'] = paths[step_params_config['software_params']['temp_path']]\n",
    "    \n",
    "    # prepare io params   \n",
    "    # input catalogs\n",
    "    input_catalogs_format = to_ext_format(step_params_config['input_params']['catalogs_format'])\n",
    "    input_catalogs_dir = paths[step_params_config['input_params']['catalogs_dir']]\n",
    "    icfs = build_filenames_list(input_images_names, new_ext=input_catalogs_format, \n",
    "                                    new_path=input_catalogs_dir)\n",
    "    \n",
    "    # output psf models\n",
    "    output_psf_models_format = to_ext_format(step_params_config['output_params']['psf_models_format'])\n",
    "    output_psf_models_dir = paths[step_params_config['output_params']['psf_models_dir']]\n",
    "    # TODO: ispect BUG in PSFEx, it replaces PSF_SUFFIX only last extension part (after last .)\n",
    "    step_params_config['software_params']['config']['PSF_SUFFIX'] = ext_split(output_psf_models_format)[-1]\n",
    "    step_params_config['software_params']['config']['PSF_DIR'] = output_psf_models_dir\n",
    "    opmfs = build_filenames_list(input_images_names, new_ext=output_psf_models_format, \n",
    "                                    new_path=output_psf_models_dir)\n",
    "    \n",
    "    # output is list of calculated psf models full pathnames\n",
    "    #\n",
    "    \n",
    "    calculate_psf_separately = False\n",
    "    if 'calculate_psf_separately' in step_params_config['output_params'].keys():\n",
    "        if step_params_config['output_params']['calculate_psf_separately'] == 'true':\n",
    "            calculate_psf_separately = True\n",
    "            \n",
    "    software_params = step_params_config['software_params'] # TODO: deepcopy or not?\n",
    "    \n",
    "    if calculate_psf_separately:\n",
    "        for icf in icfs:\n",
    "            psfex = aw.api.Astromatic(**software_params)\n",
    "            #this_cmd, kwargs2 = psfex.build_cmd(icf, **software_params)\n",
    "            #print this_cmd\n",
    "            psfex.run(icf)\n",
    "    else:\n",
    "        psfex = aw.api.Astromatic(**software_params)\n",
    "        #this_cmd, kwargs2 = psfex.build_cmd(' '.join(icfs), **software_params)\n",
    "        #print this_cmd\n",
    "        psfex.run(' '.join(icfs)) # TODO: create file with catalogs list, and pass as @catalogs_list\n",
    "    \n",
    "    return opmfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#catalog = aw.utils.ldac.get_table_from_ldac(catalogs_5[0])\n",
    "#catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Spark functions for pipeline stages</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map func for split images on groupes based on exposure type\n",
    "def split_tech_images(input_item):\n",
    "    \n",
    "    # get config from broadcast variable\n",
    "    pipeline_config = broadcast_pipeline_config.value\n",
    "    \n",
    "    # create dirs tree\n",
    "    wrk_path = str(pipeline_config['observation_id']) + '_images_preparation_map_'\n",
    "    wrk_path = wrk_path + datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S:%f')\n",
    "    dirs = AEDirsTreeConfigurer(wrk_path=wrk_path)\n",
    "    dirs.build_dirs_tree()\n",
    "    dirs.new_log_dir()\n",
    "    paths = dirs.get_paths()\n",
    "    \n",
    "    # put all config files in paths['config'] dir\n",
    "    config_files = broadcast_config_files.value\n",
    "    for config_file in config_files:\n",
    "        shutil.copyfile(SparkFiles.get(config_file), os.path.join(paths['config'], \n",
    "                                                                  os.path.basename(config_file)))\n",
    "    # save image to executor local file system\n",
    "    image_filename = os.path.join(paths['temp'], os.path.basename(input_item[0]))\n",
    "    with open(image_filename, 'wb') as fid:\n",
    "        fid.write(input_item[1])\n",
    "    exp_type = fits.getheader(image_filename, 0)[pipeline_config['exposure_type_keyword']]\n",
    "    \n",
    "    # TODO: paste here preprocessing pipeline, defined by users\n",
    "    # need to define user preproc functions interface and create dispatcher of them \n",
    "    \n",
    "    # TODO: paste this dirs tree removal into AEDirsTreeConfigurer class\n",
    "    shutil.rmtree(wrk_path, True)\n",
    "    return exp_type, input_item\n",
    "\n",
    "# (reduceByKey) for images combination to one list per each criteria \n",
    "# (i.e. observation id, it`s set in map func(s) return(s))\n",
    "def join_to_list_by_key(input_item_1, input_item_2):\n",
    "    return to_list(input_item_1) + to_list(input_item_2)\n",
    "\n",
    "# current input_items are tuples (exp_type, fits)\n",
    "# output after collect wil be a list of dicts: key=frame_type, value=list of tuples(filename, fits)\n",
    "# futher processing of techframes is done locally (it`s a simple task and needs to be done once)\n",
    "def compute_super_tech_frames_dicts(input_items):\n",
    "    frames_type = str(input_items[0])\n",
    "    res = {frames_type: []}\n",
    "    for item in input_items[1]:\n",
    "        res[frames_type].append(item)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# makes chunks of input list with size of chunk_size\n",
    "def get_chunks(input_list, chunk_size):\n",
    "    for i in xrange(0, len(input_list), chunk_size):\n",
    "        yield input_list[i:i + chunk_size]\n",
    "\n",
    "# input is a full set of images data\n",
    "# input_rdd is a tuple(exp_type, list(tuple(full_pathname, bytes)))\n",
    "def split_images_to_chunks(input_rdd):\n",
    "    chunk_size = int(broadcast_pipeline_config.value['images_split_chunk_size'])\n",
    "    input_items = input_rdd[1]\n",
    "    rdd_chunks = get_chunks(input_items, chunk_size)\n",
    "    key = 0\n",
    "    res = []\n",
    "    for chunk in rdd_chunks:\n",
    "        res.append((key, chunk))\n",
    "        key += 1\n",
    "    return res\n",
    "\n",
    "# input is a chunk of all images data\n",
    "# input_rdd is a tuple(chunk_num, list(tuple(full_pathname, bytes)))\n",
    "def calibrate_images_stage(input_rdd):\n",
    "    \n",
    "    input_items = input_rdd[1]\n",
    "    \n",
    "    # get config from broadcast variable\n",
    "    pipeline_config = broadcast_pipeline_config.value\n",
    "    \n",
    "    # create dir for dirs tree, name is name of first image in partition\n",
    "    #wrk_path = str(change_file_extension(os.path.basename(input_items[0][0]), '')) # [0][0] for full files\n",
    "    wrk_path = str(pipeline_config['observation_id']) + '_images_calibration_map_'\n",
    "    wrk_path = wrk_path + datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S:%f')\n",
    "    \n",
    "    # create dirs tree\n",
    "    dirs = AEDirsTreeConfigurer(wrk_path=wrk_path)\n",
    "    dirs.build_dirs_tree()\n",
    "    dirs.new_log_dir()\n",
    "    paths = dirs.get_paths()\n",
    "    \n",
    "    # take input images dir from json config\n",
    "    input_images_dir = paths[pipeline_config['pipeline_stages']['images_calibration']\\\n",
    "    ['stage_params']['input_params']['images_dir']]\n",
    "\n",
    "    # put all config files in paths['config'] dir\n",
    "    config_files = broadcast_config_files.value\n",
    "    for config_file in config_files:\n",
    "        shutil.copyfile(SparkFiles.get(config_file), os.path.join(paths['config'], \n",
    "                                                                  os.path.basename(config_file)))\n",
    "    # prepare a list of images names\n",
    "    # ii[0] for full files\n",
    "    input_images_names = sorted([change_file_extension(os.path.basename(ii[0]), '') for ii in input_items])\n",
    "    # check if input images list is correct\n",
    "    #with open(os.path.join(paths['temp'], 'images_list.txt'), 'w') as fid:\n",
    "        #for ii in input_images_names:\n",
    "            #fid.write('%s\\n' % ii)\n",
    "    \n",
    "    # for each item in input_items, save it to executor local file system\n",
    "    for input_item in input_items:\n",
    "        # save image to executor local file system\n",
    "        image_filename = os.path.join(paths['temp'], os.path.basename(input_item[0]))\n",
    "        with open(image_filename, 'wb') as fid:\n",
    "            fid.write(input_item[1])\n",
    "\n",
    "    # here call steps basic functions\n",
    "    # stage: images_calibration\n",
    "    catalogs_1 = create_catalogs(input_images_names, paths, pipeline_config, 'images_calibration', 'create_calibration_catalogs')\n",
    "    calculate_calibration(input_images_names, paths, pipeline_config, 'images_calibration', 'create_calibration_headers')\n",
    "    swarped_images_1, _ = swarp_images(input_images_names, paths, pipeline_config, 'images_calibration', 'calibrate_images')\n",
    "    catalogs_2 = create_catalogs(input_images_names, paths, pipeline_config, 'images_calibration', 'create_calibrated_images_catalogs')\n",
    "    calculate_psf(input_images_names, paths, pipeline_config, 'images_calibration', 'model_psf_calibrated_images_catalogs')\n",
    "    catalogs_3 = create_catalogs(input_images_names, paths, pipeline_config, 'images_calibration', 'create_calibrated_images_catalogs_psf')\n",
    "    \n",
    "    # here load result data\n",
    "    output_dict = {}\n",
    "    cal_images = {im: fits_ImageHDU_from_file(im) for im in swarped_images_1}\n",
    "    cal_cats = {cat: aw.utils.ldac.get_table_from_ldac(cat) for cat in catalogs_2}\n",
    "    cal_cats_psf = {cat: aw.utils.ldac.get_table_from_ldac(cat) for cat in catalogs_3}\n",
    "    output_dict['calibrated_images'] = cal_images\n",
    "    output_dict['calibrated_images_catalogs'] = cal_cats\n",
    "    output_dict['calibrated_images_catalogs_psf'] = cal_cats_psf\n",
    "    \n",
    "    # TODO: paste this dirs tree removal into AEDirsTreeConfigurer class\n",
    "    shutil.rmtree(wrk_path, True)\n",
    "    \n",
    "    return 1, output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# each input dict is a dict{type:dict{fn:bytes}}\n",
    "def join_to_dict_by_key(dict1, dict2):\n",
    "    res = {}\n",
    "    for k, v in dict1.iteritems():\n",
    "        if k not in res.keys():\n",
    "            res[k] = {}\n",
    "        for kk, vv in dict1[k].iteritems():\n",
    "            res[k][kk] = vv\n",
    "\n",
    "    for k, v in dict2.iteritems():\n",
    "        if k not in res.keys():\n",
    "            res[k] = {}\n",
    "        for kk, vv in dict2[k].iteritems():\n",
    "            res[k][kk] = vv\n",
    "    return res    \n",
    "\n",
    "\n",
    "# input is a dict of all calibrated images, cats anf psf_cats data\n",
    "# input_rdd is a tuple(1, dict{data_type:dict{filename:bytes}})\n",
    "def coadd_images_stage(input_rdd):\n",
    "    \n",
    "    # get json config from broadcast variable\n",
    "    pipeline_config = broadcast_pipeline_config.value\n",
    "    \n",
    "    # we need as files only calibrated images, but all input is provided futher with new data computed at this stage\n",
    "    output_dict = input_rdd[1]\n",
    "    \n",
    "    # to list of tuples, like in previuos fuctions\n",
    "    input_items = output_dict['calibrated_images'].items()\n",
    "    \n",
    "    # create dir for dirs tree, name is name of first image in partition\n",
    "    wrk_path = str(pipeline_config['observation_id']) + '_images_coaddition_map_'\n",
    "    wrk_path = wrk_path + datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S:%f')\n",
    "    \n",
    "    # create dirs tree\n",
    "    dirs = AEDirsTreeConfigurer(wrk_path=wrk_path)\n",
    "    dirs.build_dirs_tree()\n",
    "    #dirs.new_log_dir()\n",
    "    paths = dirs.get_paths()\n",
    "    \n",
    "    # take input images dir from json config\n",
    "    input_images_dir = paths[pipeline_config['pipeline_stages']['images_coaddition']\\\n",
    "    ['stage_params']['input_params']['images_dir']]\n",
    "    \n",
    "    # put all config files in paths['config'] dir\n",
    "    config_files = broadcast_config_files.value\n",
    "    for config_file in config_files:\n",
    "        shutil.copyfile(SparkFiles.get(config_file), os.path.join(paths['config'], \n",
    "                                                                  os.path.basename(config_file)))\n",
    "    \n",
    "    # prepare a list of images names\n",
    "    # ii[0] for full files\n",
    "    input_images_names = sorted([change_file_extension(os.path.basename(ii[0]), '') for ii in input_items])\n",
    "    # check if input images list is correct\n",
    "    #with open(os.path.join(paths['temp'], 'images_list.txt'), 'w') as fid:\n",
    "        #for ii in input_images_names:\n",
    "            #fid.write('%s\\n' % ii)\n",
    "    \n",
    "    # for each item in input_items, save it to executor local file system\n",
    "    for input_item in input_items:\n",
    "        fits.writeto(os.path.join(input_images_dir, os.path.basename(input_item[0])), \n",
    "                     data=input_item[1].data, header=input_item[1].header)\n",
    "    \n",
    "    # here call steps basic functions\n",
    "    # stage: images_coaddition\n",
    "    swarped_images_2, _ = swarp_images(input_images_names, paths, pipeline_config, 'images_coaddition', 'coadd_calibrated_images')\n",
    "    catalogs_4 = create_catalogs(swarped_images_2, paths, pipeline_config, 'images_coaddition', 'create_coadded_image_catalog')\n",
    "    calculate_psf(swarped_images_2, paths, pipeline_config, 'images_coaddition', 'model_psf_coadded_image_catalog')\n",
    "    catalogs_5 = create_catalogs(swarped_images_2, paths, pipeline_config, 'images_coaddition', 'create_coadded_image_catalog_psf')\n",
    "    \n",
    "    # here load result data (merge new data with data provided from above)\n",
    "    cal_images = {im: fits_ImageHDU_from_file(im) for im in swarped_images_2}\n",
    "    cal_cats = {cat: aw.utils.ldac.get_table_from_ldac(cat) for cat in catalogs_4}\n",
    "    cal_cats_psf = {cat: aw.utils.ldac.get_table_from_ldac(cat) for cat in catalogs_5}\n",
    "    \n",
    "    output_dict['calibrated_coadded_images'] = cal_images\n",
    "    output_dict['calibrated_coadded_images_catalogs'] = cal_cats\n",
    "    output_dict['calibrated_coadded_images_catalogs_psf'] = cal_cats_psf\n",
    "    \n",
    "    # TODO: paste this dirs tree removal into AEDirsTreeConfigurer class\n",
    "    shutil.rmtree(wrk_path, True)\n",
    "    \n",
    "    #for k, v in output_dict.iteritems():\n",
    "        #output_dict[k] = v.items()\n",
    "    \n",
    "    return 1, output_dict#.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_by_key(input_rdd, key):\n",
    "    input_items = input_rdd[1]\n",
    "    if key in input_items.keys():\n",
    "        res = input_items[key]\n",
    "    else:\n",
    "        res = {}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_catalogs(input_rdd, dir_name='catalogs'):\n",
    "    res =[]\n",
    "    for k, v in input_rdd.iteritems():\n",
    "        pathname = os.path.join(broadcast_paths.value[dir_name], os.path.basename(k))\n",
    "        aw.utils.ldac.save_table_as_ldac(v, pathname, clobber=True)\n",
    "        res.append(pathname)\n",
    "    return res\n",
    "\n",
    "def save_images(input_rdd, dir_name='images'):\n",
    "    res =[]\n",
    "    for k, v in input_rdd.iteritems():\n",
    "        pathname = os.path.join(broadcast_paths.value[dir_name], os.path.basename(k))\n",
    "        fits.writeto(pathname, data=v.data, header=v.header, clobber=True)\n",
    "        res.append(pathname)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Spark application main code (driver code)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init dirs tree for driver\n",
    "dirs = AEDirsTreeConfigurer()\n",
    "dirs.build_dirs_tree()\n",
    "dirs.new_log_dir()\n",
    "paths = dirs.get_paths()\n",
    "\n",
    "#load json config for pipeline\n",
    "all_configs = AEJsonConfigLoader(os.path.join(paths['config'], 'common_config.json'))\n",
    "all_configs.build_config()\n",
    "pipeline_config = all_configs.get_pipeline_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create spark configuration\n",
    "spark_conf = SparkConf().setMaster('local[4]').setAppName('PipelineTest').setSparkHome(os.environ.get('SPARK_HOME'))\n",
    "spark_conf.set('spark.pyspark.python', os.environ.get('PYSPARK_PYTHON'))\n",
    "spark_conf.set('spark.pyspark.driver.python', os.environ.get('PYSPARK_PYTHON'))\n",
    "spark_conf.set('spark.driver.memory', '4g')\n",
    "spark_conf.set('spark.files.overwrite', 'true')\n",
    "\n",
    "# create spark context\n",
    "sc = SparkContext(conf=spark_conf)\n",
    "#sc = SparkContext(conf=spark_conf, gateway=gateway)\n",
    "\n",
    "# add custom python modules to all executors\n",
    "sc.addPyFile('/home/ser/Dev/astro_engine/code/astro_utils.py')\n",
    "sc.addPyFile('/home/ser/Dev/astro_engine/code/fits_utils.py')\n",
    "\n",
    "# add config files to spark context\n",
    "config_files = get_fits_images_from_dir(paths['config'], '')\n",
    "config_files = [os.path.join(paths['config'], os.path.basename(c)) for c in config_files]\n",
    "for config_file in config_files:\n",
    "    sc.addFile(config_file)\n",
    "\n",
    "# define broadcast variables\n",
    "broadcast_paths = sc.broadcast(paths)\n",
    "broadcast_config_files = sc.broadcast(config_files)\n",
    "broadcast_pipeline_config = sc.broadcast(pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.pyspark.driver.python',\n",
       "  u'/home/ser/Dev/Python/2/anaconda2/bin/python'),\n",
       " (u'spark.master', u'local[4]'),\n",
       " (u'spark.driver.memory', u'4g'),\n",
       " (u'spark.app.name', u'PipelineTest'),\n",
       " (u'spark.pyspark.python', u'/home/ser/Dev/Python/2/anaconda2/bin/python'),\n",
       " (u'spark.executor.id', u'driver'),\n",
       " (u'spark.driver.host', u'10.0.2.15'),\n",
       " (u'spark.driver.port', u'43953'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.home', u'/home/ser/Dev/Spark/spark-2.1.0-bin-hadoop2.7'),\n",
       " (u'spark.app.id', u'local-1494982042526'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.files.overwrite', u'true')]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images RDD: PythonRDD[1] at RDD at PythonRDD.scala:48\n",
      "observation_id: GRB130427\n"
     ]
    }
   ],
   "source": [
    "images = sc.binaryFiles(paths['temp']).filter(lambda x: x[0].endswith(pipeline_config['input_images_format']))\n",
    "#images = sc.binaryFiles(\"hdfs://localhost:9000/ser/temp\")\n",
    "print 'images RDD:', images\n",
    "print 'observation_id:', broadcast_pipeline_config.value['observation_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split images on groups by image type\n",
    "images_groups = images.map(split_tech_images).reduceByKey(join_to_list_by_key)\n",
    "tech_frames = images_groups.filter(lambda x: x[0] != pipeline_config['images_exposure_type'])\n",
    "raw_images = images_groups.filter(lambda x: x[0] == pipeline_config['images_exposure_type'])\n",
    "\n",
    "# process tech_frames\n",
    "tech_frames_dicts_list = tech_frames.map(compute_super_tech_frames_dicts).collect()\n",
    "tech_frames_dict = dict([(key, d[key]) for d in tech_frames_dicts_list for key in d])\n",
    "super_frames_dict = compute_supers(tech_frames_dict, paths, pipeline_config, 'preparation', 'tech_frames_preprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: COMPUTE SUPERFRAMES (AND APPLY THEM, so raw images will be not raw) MANUALLY, FILL THIS DICT\n",
    "# super_frames_dict\n",
    "# use Pydoop for interacting with HDFS within workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# process raw_images and save results as a PickleFile\n",
    "if int(broadcast_pipeline_config.value['images_split_chunk_size']) > 0:\n",
    "    raw_images = raw_images.flatMap(split_images_to_chunks)\n",
    "result_data_rdd = raw_images.map(calibrate_images_stage).reduceByKey(join_to_dict_by_key).map(coadd_images_stage)#.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#result_data_rdd[0][1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#result_dir = 'test_result_2'\n",
    "#result_data_rdd.saveAsPickleFile(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On 16 images 512x512, local mode:<br>\n",
    "CPU times: user 748 ms, sys: 120 ms, total: 868 ms<br>\n",
    "Wall time: 1min 13s<br>\n",
    "Result size: 48.5Mb<br><br>\n",
    "On 60 images 512x512, local mode:<br>\n",
    "CPU times: user 40 ms, sys: 4 ms, total: 44 ms<br>\n",
    "Wall time: 4min 32s, 3min 56s, <br>\n",
    "Result size: 242.8Mb<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#result_dir = 'test_result'\n",
    "#new_rdd = sc.pickleFile(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_types = ['calibrated_coadded_images','calibrated_coadded_images_catalogs',\n",
    "              'calibrated_coadded_images_catalogs_psf', 'calibrated_images', \n",
    "              'calibrated_images_catalogs', 'calibrated_images_catalogs_psf']\n",
    "catalogs_types = ['calibrated_coadded_images_catalogs', 'calibrated_coadded_images_catalogs_psf',\n",
    "                  'calibrated_images_catalogs', 'calibrated_images_catalogs_psf']\n",
    "images_types = ['calibrated_coadded_images', 'calibrated_images']\n",
    "\n",
    "# cache rdd\n",
    "result_data_rdd.cache()\n",
    "\n",
    "# define rdds dict for all images\n",
    "images_rdd_dict = {}\n",
    "for image_type in images_types:\n",
    "    partial_get = partial(get_data_by_key, key=image_type)\n",
    "    images_rdd_dict[image_type] = result_data_rdd.map(partial_get)\n",
    "\n",
    "# define rdds dict for all catalogs\n",
    "catalogs_rdd_dict = {}\n",
    "for catalog_type in catalogs_types:\n",
    "    partial_get = partial(get_data_by_key, key=catalog_type)\n",
    "    catalogs_rdd_dict[catalog_type] = result_data_rdd.map(partial_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 264 ms, sys: 88 ms, total: 352 ms\n",
      "Wall time: 5min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# save catalogs and images to HDFS or local driver fyle system, store dict of paths to saved files\n",
    "catalogs_paths_dict = {}\n",
    "images_paths_dict = {}\n",
    "\n",
    "for cat_type, cat_rdd in catalogs_rdd_dict.iteritems():\n",
    "    catalogs_paths_dict[cat_type] = cat_rdd.map(save_catalogs).collect()[0]\n",
    "\n",
    "images_paths_dict['calibrated_coadded_images'] = images_rdd_dict['calibrated_coadded_images'].map(\n",
    "    lambda x: save_images(x, 'stacks')).collect()[0]\n",
    "images_paths_dict['calibrated_images'] = images_rdd_dict['calibrated_images'].map(save_images).collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 images 512x512<br>\n",
    "CPU times: user 176 ms, sys: 12 ms, total: 188 ms<br>\n",
    "Wall time: 9.77 s<br>\n",
    "60 images 512x512<br>\n",
    "CPU times: user 264 ms, sys: 88 ms, total: 352 ms<br>\n",
    "Wall time: 5min 43s<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1208"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print files.map(extract).reduce(join_cats)\n",
    "#files.map(partial(extract_1, broadcast_fits=broadcast_fits)).map(lambda x: [x]).reduce(lambda a, b: a + b)\n",
    "#files.mapPartitions(rt_1).map(lambda x : [x]).reduce(lambda a, b: a + b)\n",
    "#catalogs = files.map(extract_3).collect()\n",
    "#len(catalogs)\n",
    "#catalogs.map(lambda x: [x]).reduce(lambda a, b: a + b)\n",
    "#catalogs.saveAsSequenceFile(cfg.get_paths()['catalogs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppp = os.path.join(paths['catalogs'], 'GRB130427A-60sec-001_001.cat')\n",
    "#catalog = aw.utils.ldac.get_table_from_ldac(catalogs_5[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:iraf27]",
   "language": "python",
   "name": "conda-env-iraf27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
