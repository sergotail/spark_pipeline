{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare invironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import fnmatch\n",
    "import re\n",
    "import astromatic_wrapper as aw #TODO: install aw to itaf virtual env\n",
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyraf.iraf as iraf\n",
    "import pyraf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/ser/Dev/astro_engine/code')\n",
    "from astro_utils import AEDirsTreeConfigurer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fits_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_iraf_tasks = 0\n",
    "iraf.module.images(_doprint=print_iraf_tasks) # print or not to print (why it not working?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just test that PyRAF works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print iraf.module.imaccess('im_3.fit') #TODO: implement access check if needed\n",
    "input_fits = 'im_1.fit, im_2.fit'\n",
    "output_fits = 'res.fit'\n",
    "s = iraf.module.imcombine(input_fits, output_fits, combine='median', Stdout=1, Stderr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'catalogs': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/catalogs',\n",
       " 'config': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/config',\n",
       " 'images': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/images',\n",
       " 'logs': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/logs',\n",
       " 'stacks': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/stacks',\n",
       " 'temp': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/temp'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs = AEDirsTreeConfigurer()#images_path='images/GRB130427A')\n",
    "dirs.build_dirs_tree()\n",
    "dirs.new_log_dir()\n",
    "paths = dirs.get_paths()\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set image filename\n",
    "files = {'image': os.path.join(paths['images'], 'GRB130427_R60_001_003.fit')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# correct flats headers\n",
    "#from functools import partial\n",
    "#p = '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/temp/flat'\n",
    "#f = filter_fits_by_header(input_images_names, paths['temp']+'/flat', EXPTYPE='Light')\n",
    "#change_header_apply = partial(change_header, fits_path=p, EXPTYPE=to_header_str_format('Flat'))\n",
    "#apply_to_fits(f, p, change_header_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take lists of each type images\n",
    "input_images_names = sorted(get_fits_images_from_dir(os.path.join(paths['temp']), 'fit', True, True))\n",
    "bias_list = filter_fits_by_header(input_images_names, paths['temp'], EXPTYPE='Bias')\n",
    "dark_list = filter_fits_by_header(input_images_names, paths['temp'], EXPTYPE='Dark')\n",
    "flat_list = filter_fits_by_header(input_images_names, paths['temp'], EXPTYPE='Flat')\n",
    "images_list = filter_fits_by_header(input_images_names, paths['temp'], EXPTYPE='Light')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# superbias\n",
    "iraf.noao.imred(_doprint=0)\n",
    "iraf.noao.imred.ccdred(_doprint=0)\n",
    "iraf.noao.imred.ccdred.instrument = os.path.join(paths['config'], 'instrument.dat')\n",
    "iraf.noao.imred.ccdred.zerocombine.setParam('ccdtype', \"\")  \n",
    "gain = fits.getheader(bias_list[0], 0)['GAIN']\n",
    "iraf.noao.imred.ccdred.zerocombine.setParam('gain', gain) \n",
    "bias_list_txt = os.path.join(paths['temp'], 'super', 'bias_list.txt')\n",
    "list_to_file(bias_list, bias_list_txt)\n",
    "superbias_filename = os.path.join(paths['temp'], 'super', 'superbias.fits')\n",
    "iraf.noao.imred.ccdred.zerocombine(input='@' + bias_list_txt, output=superbias_filename, \n",
    "                                       process='no', delete='no', clobber='no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darks exp times: [2.0, 60.0]\n",
      "filter: R\n",
      "flats exp times: []\n",
      "images exp times: [60.0]\n",
      "best darks exp time for images: {60.0: 60.0}\n",
      "best darks exp time for flats: {}\n",
      "best flats exp time for images: {}\n"
     ]
    }
   ],
   "source": [
    "# compute darks_exp_times, key:filename, value:exp_time\n",
    "darks_exp_times_dict = header_keywords_values(dark_list, keywords=['EXPTIME'])\n",
    "if 'EXPTIME' in darks_exp_times_dict.keys():\n",
    "    darks_exp_times = sorted(darks_exp_times_dict['EXPTIME'].keys())\n",
    "else:\n",
    "    darks_exp_times = []\n",
    "print 'darks exp times:', darks_exp_times\n",
    "\n",
    "# for each image filter, find best dark for flat and flat for image\n",
    "filters = ['R']\n",
    "filters_exp_times_mappings = {}\n",
    "for filter_ in filters:\n",
    "    print 'filter:', filter_\n",
    "    # filter fits with current filter\n",
    "    filter_flat_list = filter_fits_by_header(flat_list, paths['temp'], FILTER=filter_)\n",
    "    filter_images_list = filter_fits_by_header(images_list, paths['temp'], FILTER=filter_)\n",
    "    \n",
    "    # compute flats_exp_times\n",
    "    flats_exp_times_dict = header_keywords_values(filter_flat_list, keywords=['EXPTIME'])\n",
    "    if 'EXPTIME' in flats_exp_times_dict.keys():\n",
    "        flats_exp_times = sorted(flats_exp_times_dict['EXPTIME'].keys())\n",
    "    else:\n",
    "        flats_exp_times = []\n",
    "    print 'flats exp times:', flats_exp_times\n",
    "\n",
    "    # compute images_exp_times\n",
    "    images_exp_times_dict = header_keywords_values(filter_images_list, keywords=['EXPTIME'])\n",
    "    if 'EXPTIME' in images_exp_times_dict.keys():\n",
    "        images_exp_times = sorted(images_exp_times_dict['EXPTIME'].keys())\n",
    "    else:\n",
    "        images_exp_times = []\n",
    "    print 'images exp times:', images_exp_times\n",
    "\n",
    "    # search for best dark_exp_times for images\n",
    "    images_darks_exp_times = best_exp_time(images_exp_times, darks_exp_times)\n",
    "    print \"best darks exp time for images:\", images_darks_exp_times\n",
    "\n",
    "    # search for best dark_exp_times for flats\n",
    "    flat_darks_exp_times = best_exp_time(flats_exp_times, darks_exp_times)\n",
    "    print \"best darks exp time for flats:\", flat_darks_exp_times\n",
    "\n",
    "    # search for best flat_exp_times for images\n",
    "    images_flats_exp_times = best_exp_time(images_exp_times, flats_exp_times)\n",
    "    print \"best flats exp time for images:\", images_flats_exp_times\n",
    "    \n",
    "    filters_exp_times_mappings[filter_] = {'images-dark': images_darks_exp_times, \n",
    "                                           'flat-dark': flat_darks_exp_times,\n",
    "                                           'images-flat': images_flats_exp_times}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R': {'flat-dark': {}, 'images-dark': {60.0: 60.0}, 'images-flat': {}}}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters_exp_times_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/temp/super/superdark_R_images_60.fits is not an image or a number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter: R\n",
      "use dark with exp_time 60.0 for images with exp_time 60.0 coeff = 1.000000\n",
      "don't use flat for images for this filter\n",
      "don't use dark for flat for this filter\n"
     ]
    }
   ],
   "source": [
    "# when we know which dark needed for which flat and which image, so compute all needed darks\n",
    "exp_times_superdarks = {}\n",
    "for filter_, mapps in filters_exp_times_mappings.iteritems():\n",
    "    for mapp, vals in mapps.iteritems():\n",
    "        for_, by_ = mapp.split('-')[0], mapp.split('-')[1]\n",
    "        if vals and by_ == 'dark':\n",
    "            for k, v in vals.iteritems():\n",
    "                superdark_filename = os.path.join(paths['temp'], 'super',\n",
    "                                                  'superdark_%s_%s_%d.fits' % (filter_, for_, k))\n",
    "                input_darks_list = darks_exp_times_dict['EXPTIME'][v]\n",
    "                scale_coeff = float(k) / float(v)\n",
    "                iraf.noao.imred(_doprint=0)\n",
    "                iraf.noao.imred.ccdred(_doprint=0)\n",
    "                iraf.noao.imred.ccdred.instrument = os.path.join(paths['config'], 'instrument.dat')\n",
    "                iraf.noao.imred.ccdred.darkcombine.setParam('ccdtype', \"\")  \n",
    "                gain = fits.getheader(input_darks_list[0], 0)['GAIN']\n",
    "                iraf.noao.imred.ccdred.darkcombine.setParam('gain', gain)\n",
    "                iraf.noao.imred.ccdred.darkcombine.setParam('scale', 'exposure')\n",
    "                iraf.noao.imred.ccdred.darkcombine.setParam('combine', 'median')\n",
    "                iraf.noao.imred.ccdred.darkcombine.setParam('process', 'no')\n",
    "                dark_list_txt = os.path.join(paths['temp'], 'super', 'dark_list.txt')\n",
    "                list_to_file(input_darks_list, dark_list_txt)\n",
    "                iraf.noao.imred.ccdred.darkcombine(input='@' + dark_list_txt, output=superdark_filename)\n",
    "                # TODO: subtract superbias\n",
    "                \n",
    "                #iraf.module.imarith.lParam()\n",
    "                sub_1 = 'temp/super/superdark.txt'\n",
    "                sub_2 = 'temp/super/superbias.txt'\n",
    "                list_to_file([superdark_filename], sub_1)\n",
    "                list_to_file([superbias_filename], sub_2)\n",
    "                iraf.module.imarith('@'+sub_1, '-', '@'+sub_2, 'res.fits')\n",
    "                \n",
    "# \n",
    "for filter_, mapps in filters_exp_times_mappings.iteritems():\n",
    "    print 'filter:', filter_\n",
    "    for mapp, vals in mapps.iteritems():\n",
    "        for_, by_ = mapp.split('-')[0], mapp.split('-')[1]\n",
    "        if not vals:\n",
    "            print 'don\\'t use', by_, 'for', for_, 'for this filter'\n",
    "        else:\n",
    "            for k, v in vals.iteritems():\n",
    "                coef_str = 'coeff = %f' %(float(k)/float(v)) if by_ == 'dark' else ''\n",
    "                print 'use', by_, 'with exp_time', v, 'for', for_, 'with exp_time', k, coef_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2.0: 12, 60.0: 60}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute superdark for each flat exp_times\n",
    "for exp_time in \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_exp_time(exp_list_for, exp_list_by):\n",
    "    res = {}\n",
    "    if not exp_list_by or not exp_list_for:\n",
    "        return res\n",
    "    for iet in exp_list_for:\n",
    "        min_diff = abs(exp_list_by[0] - iet)\n",
    "        best_det = exp_list_by[0]\n",
    "        for det in exp_list_by:\n",
    "            diff = abs(det - iet)\n",
    "            if diff < min_diff:\n",
    "                best_det = det\n",
    "                min_diff = diff\n",
    "        res[iet] = best_det\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# superdarks\n",
    "exp_times = ['2.', '60.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pipeline (local version)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import all needed packages</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import copy\n",
    "import astromatic_wrapper as aw #TODO: install aw to itaf virtual env\n",
    "from astropy.io import fits\n",
    "import pyraf\n",
    "from pyraf import iraf\n",
    "import sys\n",
    "sys.path.insert(0, '/home/ser/Dev/astro_engine/code')\n",
    "from astro_utils import AEDirsTreeConfigurer, AEJsonConfigLoader\n",
    "from fits_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define pipeline parameters</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dirs tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'catalogs': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/catalogs',\n",
       " 'config': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/config',\n",
       " 'images': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/images',\n",
       " 'logs': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/logs',\n",
       " 'stacks': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/stacks',\n",
       " 'temp': '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/temp'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs = AEDirsTreeConfigurer()#images_path='images/GRB130427A')\n",
    "dirs.build_dirs_tree()\n",
    "dirs.new_log_dir()\n",
    "paths = dirs.get_paths()\n",
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define config for astro software from json config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config_files': ['pipeline_config']}\n"
     ]
    }
   ],
   "source": [
    "all_configs = AEJsonConfigLoader(os.path.join(paths['config'], 'common_config.json'))\n",
    "print all_configs.get_required_common_params()\n",
    "all_configs.build_config()\n",
    "pipeline_config = all_configs.get_pipeline_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- preparation\n",
      "fits_preprocessing_common\n",
      "tech_frames_preprocessing\n",
      "-----------\n",
      "----- images_calibration\n",
      "create_calibration_headers\n",
      "basic_reduction\n",
      "create_calibrated_images_catalogs\n",
      "calibrate_images\n",
      "model_psf_calibrated_images_catalogs\n",
      "create_calibration_catalogs\n",
      "create_calibrated_images_catalogs_psf\n",
      "-----------\n",
      "----- images_coaddition\n",
      "model_psf_coadded_image_catalog\n",
      "coadd_calibrated_images\n",
      "create_coadded_image_catalog\n",
      "create_coadded_image_catalog_psf\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "for key in pipeline_config['pipeline_stages'].keys():\n",
    "    print \"-----\",key\n",
    "    for kkey in pipeline_config['pipeline_stages'][key]['stage_steps'].keys():\n",
    "        print kkey\n",
    "    print '-----------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EXPTYPE - FITS header keywords for type of frame\n",
    "#Bias - bias\n",
    "#Dark - dark\n",
    "#Light - flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.dev\n"
     ]
    }
   ],
   "source": [
    "print(pyraf.__version__)\n",
    "#iraf.pwd()\n",
    "#from pyraf import irafinst\n",
    "#irafinst.getIrafVer()\n",
    "\n",
    "#iraf.files(bias_list)\n",
    "\n",
    "#iraf.images(_doprint=0) # load images\n",
    "#iraf.imutil(_doprint=0) # load imutil\n",
    "#iraf.imheader.setParam('longheader', 'yes')\n",
    "#iraf.imheader(bias_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def list_to_file(list_, filename, sep='\\n'):\n",
    "    with open(filename, 'w') as fid:\n",
    "        for item in list_:\n",
    "            fid.write('%s%s' % (str(item), sep))\n",
    "\n",
    "def to_list(something):\n",
    "    return something if isinstance(something, list) else [something]\n",
    "\n",
    "def filter_fits_by_header(fits_filenames, fits_path=None, match_any=False, ignore_case=True,\n",
    "                          ignore_edge_spaces=True, hdu_num=0, **kwargs):\n",
    "    iffs = [os.path.join(fits_path, os.path.basename(iff)) if fits_path is not None else iff\\\n",
    "            for iff in to_list(fits_filenames)]\n",
    "    res = []\n",
    "    for iff in iffs:\n",
    "        fits_file_header = fits.getheader(iff, hdu_num)\n",
    "        keywords_match = 0\n",
    "        for keyword, value in kwargs.iteritems():\n",
    "            if keyword in fits_file_header:\n",
    "                header_val = str(fits_file_header[keyword]).strip() if ignore_edge_spaces \\\n",
    "                else str(fits_file_header[keyword])\n",
    "                header_val = header_val.lower() if ignore_case else header_val\n",
    "                kwargs_val = str(value).strip() if ignore_edge_spaces else str(value)\n",
    "                kwargs_val = kwargs_val.lower() if ignore_case else kwargs_val\n",
    "                if header_val == kwargs_val:\n",
    "                    keywords_match += 1\n",
    "                    if match_any:\n",
    "                        break\n",
    "        if (keywords_match and match_any) or (keywords_match == len(kwargs)):\n",
    "            res.append(iff)\n",
    "    return res\n",
    "\n",
    "def header_keywords_values(fits_filenames, fits_path=None, keywords=['SIMPLE'], keys_keywords=True, lower_case=True,\n",
    "                           split_edge_spaces=True, hdu_num=0):\n",
    "    iffs = [os.path.join(fits_path, os.path.basename(iff)) if fits_path is not None else iff \\\n",
    "            for iff in to_list(fits_filenames)]\n",
    "    kws = to_list(keywords)\n",
    "    res = {}\n",
    "    for iff in iffs:\n",
    "        fits_file_header = fits.getheader(iff, hdu_num)\n",
    "        if keys_keywords:\n",
    "            for keyword in kws:\n",
    "                if keyword not in res.keys():\n",
    "                    res[keyword] = {}\n",
    "                if keyword in fits_file_header:\n",
    "                    val = fits_file_header[keyword]\n",
    "                    if val not in res[keyword].keys():\n",
    "                        res[keyword][val] = []\n",
    "                    res[keyword][val].append(iff)\n",
    "        else:   \n",
    "            for keyword in kws:\n",
    "                if keyword in fits_file_header:\n",
    "                    res[iff] = {keyword: fits_file_header[keyword]}\n",
    "                else:\n",
    "                    res[iff] = {keyword: None}\n",
    "    return res\n",
    "\n",
    "def change_header(fits_filename, fits_path=None, new_path=None, clobber=True, **kwargs):\n",
    "    iff = os.path.join(fits_path, os.path.basename(fits_filename)) if fits_path is not None else fits_filename\n",
    "    data, header = fits.getdata(iff, header=True)\n",
    "    for keyword, value in kwargs.iteritems():\n",
    "        header[keyword] = value\n",
    "    if new_path is None:\n",
    "        new_path = fits_path\n",
    "    iff = os.path.join(new_path, os.path.basename(iff))\n",
    "    fits.writeto(iff, data, header, clobber=clobber)\n",
    "    return iff\n",
    "\n",
    "def to_header_str_format(str_value, length=8):\n",
    "    return str_value.strip().ljust(length)\n",
    "\n",
    "def apply_to_fits(fits_filenames, fits_path=None, func=None):\n",
    "    iffs = [os.path.join(fits_path, os.path.basename(iff)) if fits_path is not None else iff \\\n",
    "            for iff in to_list(fits_filenames)]\n",
    "    if not func:\n",
    "        return iffs\n",
    "    res = []\n",
    "    for iff in iffs:\n",
    "        res.append(func(iff))\n",
    "    return res\n",
    "\n",
    "def images_stat(images, fields='mode,mean', return_dict=True, stdout=1, stderr=2):\n",
    "    iraf.images(_doprint=0)\n",
    "    if return_dict:\n",
    "        iraf.images.imstat.setParam('format', 'no')\n",
    "        #iraf.images.imstat.setParam('')\n",
    "    iraf.images.imstat.setParam('fields', fields)\n",
    "    images = images if isinstance(images, list) else [images]\n",
    "    res = []\n",
    "    for image in images:\n",
    "        res.append(iraf.imstat(image, Stdout=stdout, Stderr=stderr))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from functools import partial\n",
    "#p = '/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/temp'\n",
    "#f = filter_fits_by_header(input_images_names, paths['temp'], EXPTYPE='DARK')\n",
    "#change_header_apply = partial(change_header, fits_path=p, EXPTYPE=to_header_str_format('Bias'))\n",
    "#apply_to_fits(f, p, change_header_apply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Basic reduction</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_superbias(input_images_names, paths, pipeline_config, pipeline_stage, pipeline_step):\n",
    "    # TODO: check config, stage and step for needed for this function keys\n",
    "    # select pipeline stage and step in config\n",
    "    step_config = copy.deepcopy(pipeline_config['pipeline_stages'][pipeline_stage]['stage_steps'][pipeline_step])\n",
    "    step_params_config = step_config['step_params']\n",
    "    software_params = step_params_config['software_params']\n",
    "    \n",
    "    input_tech_frames_dir = paths[step_params_config['input_params']['tech_frames']['frames_dir']]\n",
    "    output_tech_frames_dir = paths[step_params_config['output_params']['tech_frames']['frames_dir']]\n",
    "\n",
    "    input_bias_config = step_params_config['input_params']['tech_frames']['bias']\n",
    "    input_bias_config['format'] = to_ext_format(input_bias_config['format'])\n",
    "    output_bias_config = step_params_config['output_params']['tech_frames']['bias']\n",
    "    output_bias_config['format'] = to_ext_format(output_bias_config['format'])\n",
    "    \n",
    "    input_bias_dir = os.path.join(input_tech_frames_dir, input_bias_config['sub_dir'])\n",
    "    output_bias_dir = os.path.join(output_tech_frames_dir, output_bias_config['sub_dir'])\n",
    "\n",
    "    superbias_filename = os.path.join(output_bias_dir,\n",
    "                                      (output_bias_config['name'] + output_bias_config['format']))\n",
    "\n",
    "    filter_bias_kwargs = {input_bias_config['type_keyword']: input_bias_config['type_keyword_value']}\n",
    "    input_bias_filenames = filter_fits_by_header(input_images_names, input_bias_dir, \n",
    "                                                 **filter_bias_kwargs)\n",
    "    \n",
    "    iraf.noao.imred(_doprint=0)\n",
    "    iraf.noao.imred.ccdred(_doprint=0)\n",
    "    iraf.noao.imred.ccdred.instrument = os.path.join(paths[software_params['config']['INSTRUMENT_DIR']],\n",
    "                                                     software_params['config']['INSTRUMENT_FILE'])\n",
    "    \n",
    "    if 'ccdtype' in input_bias_config.keys():\n",
    "        iraf.noao.imred.ccdred.zerocombine.setParam('ccdtype', input_bias_config['ccdtype'])\n",
    "    else:\n",
    "        iraf.noao.imred.ccdred.zerocombine.setParam('ccdtype', '')\n",
    "    if 'gain_keyword' in input_bias_config.keys():\n",
    "        gain = fits.getheader(input_bias_filenames[0], 0)[input_bias_config['gain_keyword']]\n",
    "        iraf.noao.imred.ccdred.zerocombine.setParam('gain', gain)\n",
    "    \n",
    "    bias_list = os.path.join(paths['temp'], 'bias_list.txt')\n",
    "    list_to_file(input_bias_filenames, bias_list)\n",
    "    \n",
    "    iraf.noao.imred.ccdred.zerocombine(input='@' + bias_list, output=superbias_filename, \n",
    "                                       process='no', delete='no', clobber='no')\n",
    "    \n",
    "    return superbias_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_supers(input_images_names, paths, pipeline_config, pipeline_stage, pipeline_step):\n",
    "    # TODO: check config, stage and step for needed for this function keys\n",
    "    # select pipeline stage and step in config\n",
    "    step_config = copy.deepcopy(pipeline_config['pipeline_stages'][pipeline_stage]['stage_steps'][pipeline_step])\n",
    "    step_params_config = step_config['step_params']\n",
    "    input_params_config = step_params_config['input_params']\n",
    "    output_params_config = step_params_config['output_params']\n",
    "    \n",
    "    # prepare io params \n",
    "    exp_times = to_list(input_params_config['images_exp_times_secs'])\n",
    "    filters = to_list(input_params_config['images_filters'])\n",
    "    images_filter_keyword = input_params_config['images_filter_keyword']\n",
    "    input_tech_frames_dir = paths[input_params_config['tech_frames']['frames_dir']]\n",
    "    output_tech_frames_dir = paths[output_params_config['tech_frames']['frames_dir']]\n",
    "    use_bias = 'bias' in input_params_config['tech_frames']['use_frames']\n",
    "    use_dark = 'dark' in input_params_config['tech_frames']['use_frames']\n",
    "    use_flat = 'flat' in input_params_config['tech_frames']['use_frames']\n",
    "\n",
    "    \n",
    "    if use_bias:\n",
    "        superbias_filename = compute_superbias(input_images_names, paths, \n",
    "                                               pipeline_config, pipeline_stage, pipeline_step)   \n",
    "    if use_dark:\n",
    "        # compute superdarks for all exp_times for data images\n",
    "        #if no exp_time, take exp_time with biggest list of corresponding darks\n",
    "        exp_times_darks = {}\n",
    "        for exp_time in exp_times:\n",
    "            exp_times_darks[exp_time] = compute_superdark(input_images_names, paths, \n",
    "                                               pipeline_config, pipeline_stage, pipeline_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input_images_names = sorted(get_fits_images_from_dir(os.path.join(paths['temp']), 'fit', True, True))\n",
    "#compute_supers(input_images_names, paths, pipeline_config, 'preparation', 'tech_frames_preprocessing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Catalog extraction basic function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_catalogs(input_images_names, paths, pipeline_config, pipeline_stage, pipeline_step):\n",
    "    # TODO: check config, stage and step for needed for this function keys\n",
    "    # select pipeline stage and step in config\n",
    "    step_config = copy.deepcopy(pipeline_config['pipeline_stages'][pipeline_stage]['stage_steps'][pipeline_step])\n",
    "    step_params_config = step_config['step_params']\n",
    "    \n",
    "    # use paths to fill path-depended params TODO: preprocess config to do this\n",
    "    step_params_config['software_params']['config_file'] = os.path.join(paths['config'],\n",
    "                                                                step_params_config['software_params']['config_file'])\n",
    "    step_params_config['software_params']['temp_path'] = paths[step_params_config['software_params']['temp_path']]\n",
    "    step_params_config['software_params']['config']['FILTER_NAME'] = os.path.join(paths['config'],\n",
    "                                                      step_params_config['software_params']['config']['FILTER_NAME'])\n",
    "    \n",
    "    # prepare io params   \n",
    "    # input images\n",
    "    input_images_format = to_ext_format(step_params_config['input_params']['images_format'])\n",
    "    input_images_dir = paths[step_params_config['input_params']['images_dir']]\n",
    "    iffs = build_filenames_list(input_images_names, new_ext=input_images_format, \n",
    "                                new_path=input_images_dir)\n",
    "    \n",
    "    # input psf models, if exist\n",
    "    look_for_psf = False\n",
    "    ipfs = range(len(iffs))\n",
    "    if 'psf_models_format' in step_params_config['input_params'].keys() and \\\n",
    "    'psf_models_dir' in step_params_config['input_params'].keys():\n",
    "        look_for_psf = True\n",
    "        input_psf_models_format = to_ext_format(step_params_config['input_params']['psf_models_format'])\n",
    "        input_psf_models_dir = paths[step_params_config['input_params']['psf_models_dir']]\n",
    "        ipfs = build_filenames_list(input_images_names, new_ext=input_psf_models_format, \n",
    "                                    new_path=input_psf_models_dir)\n",
    "    \n",
    "    # output catalogs\n",
    "    output_catalogs_format = to_ext_format(step_params_config['output_params']['catalogs_format'])\n",
    "    output_catalogs_dir = paths[step_params_config['output_params']['catalogs_dir']]\n",
    "    ocfs = build_filenames_list(input_images_names, new_ext=output_catalogs_format, \n",
    "                                    new_path=output_catalogs_dir)\n",
    "    \n",
    "    \n",
    "    if 'check_params_for_scamp_required' in step_params_config['output_params'].keys():\n",
    "        if step_params_config['output_params']['check_params_for_scamp_required'] == 'true':\n",
    "            missing_req_scamp_params = list(set(pipeline_config['scamp_required_catalog_params']) - \\\n",
    "            set(step_params_config['software_params']['params']))\n",
    "            step_params_config['software_params']['params'].extend(missing_req_scamp_params)\n",
    "    \n",
    "    if 'check_params_for_psfex_required' in step_params_config['output_params'].keys():\n",
    "        if step_params_config['output_params']['check_params_for_psfex_required'] == 'true':\n",
    "            missing_req_psfex_params = list(set(pipeline_config['psfex_required_catalog_params']) - \\\n",
    "            set(step_params_config['software_params']['params']))\n",
    "            step_params_config['software_params']['params'].extend(missing_req_psfex_params)\n",
    "    \n",
    "    # output is a list of catalogs full pathnames\n",
    "    catalogs = []\n",
    "    \n",
    "    for (iff, ipf, ocf) in zip(iffs, ipfs, ocfs):\n",
    "        software_params = step_params_config['software_params'] # TODO: deepcopy or not?\n",
    "        # TODO: is it needed or SExtractor recognizes it by itself?\n",
    "        if 'params_from_fits_header' in step_params_config['data_handle'].keys():\n",
    "            fits_file = fits.open(iff)\n",
    "            fits_file_header = fits_file[0].header\n",
    "            for param_name, fits_header_keyword in step_params_config['data_handle']['params_from_fits_header'].iteritems():\n",
    "                if fits_header_keyword in fits_file_header:\n",
    "                    software_params['config'][param_name] = str(fits_file[0].header[fits_header_keyword])\n",
    "        software_params['config']['CATALOG_NAME'] = ocf\n",
    "        if look_for_psf:\n",
    "            software_params['config']['PSF_NAME'] = ipf\n",
    "        sextractor = aw.api.Astromatic(**software_params)\n",
    "        #this_cmd, kwargs2 = sextractor.build_cmd(iff, **software_params)\n",
    "        #print this_cmd\n",
    "        sextractor.run(iff) #TODO: handle run status (maybe return it?)\n",
    "        catalogs.append(ocf)\n",
    "    \n",
    "    return catalogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Calculate calibration basic function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_calibration(input_images_names, paths, pipeline_config, pipeline_stage, pipeline_step):\n",
    "    # TODO: check config, stage and step for needed for this function keys\n",
    "    # select pipeline stage and step in config\n",
    "    step_config = copy.deepcopy(pipeline_config['pipeline_stages'][pipeline_stage]['stage_steps'][pipeline_step])\n",
    "    step_params_config = step_config['step_params']\n",
    "    \n",
    "    # use paths to fill path-depended params TODO: preprocess config to do this\n",
    "    step_params_config['software_params']['config_file'] = os.path.join(paths['config'],\n",
    "                                                                step_params_config['software_params']['config_file'])\n",
    "    step_params_config['software_params']['temp_path'] = paths[step_params_config['software_params']['temp_path']]\n",
    "    \n",
    "    # prepare io params   \n",
    "    # input catalogs\n",
    "    input_catalogs_format = to_ext_format(step_params_config['input_params']['catalogs_format'])\n",
    "    input_catalogs_dir = paths[step_params_config['input_params']['catalogs_dir']]\n",
    "    icfs = build_filenames_list(input_images_names, new_ext=input_catalogs_format, \n",
    "                                    new_path=input_catalogs_dir)\n",
    "    \n",
    "    # output is empty list\n",
    "    result = []\n",
    "    \n",
    "    calculate_calibration_separately = False\n",
    "    if 'calculate_calibration_separately' in step_params_config['output_params'].keys():\n",
    "        if step_params_config['output_params']['calculate_calibration_separately'] == 'true':\n",
    "            calculate_calibration_separately = True\n",
    "            \n",
    "    software_params = step_params_config['software_params'] # TODO: deepcopy or not?\n",
    "    \n",
    "    if calculate_calibration_separately:\n",
    "        for icf in icfs:\n",
    "            scamp = aw.api.Astromatic(**software_params)\n",
    "            #this_cmd, kwargs2 = scamp.build_cmd(icf, **software_params)\n",
    "            #print this_cmd\n",
    "            scamp.run(icf)\n",
    "    else:\n",
    "        scamp = aw.api.Astromatic(**software_params)\n",
    "        scamp.run(' '.join(icfs)) # TODO: create file with catalogs list, and pass as @catalogs_list\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Image SWarp basic function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from itertools import *\n",
    "def swarp_images(input_images_names, paths, pipeline_config, pipeline_stage, pipeline_step):\n",
    "    # TODO: check config, stage and step for needed for this function keys\n",
    "    # select pipeline stage and step in config\n",
    "    step_config = copy.deepcopy(pipeline_config['pipeline_stages'][pipeline_stage]['stage_steps'][pipeline_step])\n",
    "    step_params_config = step_config['step_params']\n",
    "    \n",
    "    # use paths to fill path-depended params TODO: preprocess config to do this\n",
    "    step_params_config['software_params']['config_file'] = os.path.join(paths['config'],\n",
    "                                                                step_params_config['software_params']['config_file'])\n",
    "    step_params_config['software_params']['temp_path'] = paths[step_params_config['software_params']['temp_path']]\n",
    "    step_params_config['software_params']['config']['RESAMPLE_DIR'] = paths[step_params_config['software_params']['config']['RESAMPLE_DIR']]\n",
    "    \n",
    "    # prepare io params\n",
    "    # input images\n",
    "    input_images_format = to_ext_format(step_params_config['input_params']['images_format'])\n",
    "    input_images_dir = paths[step_params_config['input_params']['images_dir']]\n",
    "    iffs = build_filenames_list(input_images_names, new_ext=input_images_format, \n",
    "                                new_path=input_images_dir)\n",
    "    \n",
    "    # output images\n",
    "    output_images_format = to_ext_format(step_params_config['output_params']['images_format'])\n",
    "    output_images_dir = paths[step_params_config['output_params']['images_dir']]\n",
    "    \n",
    "    # output weightmaps\n",
    "    output_weightmaps_format = to_ext_format(step_params_config['output_params']['weightmaps_format'])\n",
    "    output_weightmaps_dir = paths[step_params_config['output_params']['weightmaps_dir']]\n",
    "    \n",
    "    # output is a tuple of lists: swarped images full pathnames and weightmaps full pathnames\n",
    "    swarped_images, weightmaps = [], []\n",
    "    \n",
    "    swarp_separately = False\n",
    "    if 'swarp_separately' in step_params_config['output_params'].keys():\n",
    "        if step_params_config['output_params']['swarp_separately'] == 'true':\n",
    "            swarp_separately = True\n",
    "            \n",
    "    software_params = step_params_config['software_params'] # TODO: deepcopy or not?\n",
    "                   \n",
    "    if swarp_separately:\n",
    "        # prepare output_images_filenames and output_weightmaps_filenames (convert them to full pathname)\n",
    "        offs = build_filenames_list(input_images_names, new_ext=output_images_format, \n",
    "                                    new_path=output_images_dir)\n",
    "        owmfs = build_filenames_list(input_images_names, new_ext=output_weightmaps_format, \n",
    "                                    new_path=output_weightmaps_dir)\n",
    "        \n",
    "        for (iff, off, owmf) in zip(iffs, offs, owmfs):\n",
    "            software_params['config']['IMAGEOUT_NAME'] = off\n",
    "            software_params['config']['WEIGHTOUT_NAME'] = owmf\n",
    "            swarp = aw.api.Astromatic(**software_params)\n",
    "            swarp.run(iff)\n",
    "            swarped_images.append(off)\n",
    "            weightmaps.append(owmf)   \n",
    "    else:\n",
    "        # build coadded image and weightmap names\n",
    "        output_filename = pipeline_config['observation_id']\n",
    "        coadd_images_nums = sorted(\n",
    "            [os.path.basename(iff).rstrip(input_images_format).rsplit('_',1)[1] for iff in iffs])\n",
    "        nums_ranges = []\n",
    "        for k, g in groupby(enumerate(coadd_images_nums), lambda (i,x):int(i)-int(x)):\n",
    "            nums_range = map(itemgetter(1),g)\n",
    "            nums_ranges.append(nums_range[0] if len(nums_range) == 1 else nums_range[0] + '-' + nums_range[-1])\n",
    "        nums_ranges = '_'.join(nums_ranges)\n",
    "        output_image_name = output_filename + '_' + nums_ranges + output_images_format\n",
    "        output_image_name = os.path.join(output_images_dir, output_image_name)\n",
    "        output_weightmap_name = output_filename + '_' + nums_ranges + output_weightmaps_format\n",
    "        output_weightmap_name = os.path.join(output_weightmaps_dir, output_weightmap_name)\n",
    "        # run SWarp\n",
    "        software_params['config']['IMAGEOUT_NAME'] = output_image_name\n",
    "        software_params['config']['WEIGHTOUT_NAME'] = output_weightmap_name\n",
    "        swarp = aw.api.Astromatic(**software_params)\n",
    "        swarp.run(' '.join(iffs)) # TODO: create file with images list, and pass as @images_list\n",
    "        swarped_images.append(output_image_name)\n",
    "        weightmaps.append(output_weightmap_name)\n",
    "    \n",
    "    return swarped_images, weightmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Calculate PSF basic function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_psf(input_images_names, paths, pipeline_config, pipeline_stage, pipeline_step):\n",
    "    # TODO: check config, stage and step for needed for this function keys\n",
    "    # select pipeline stage and step in config\n",
    "    step_config = copy.deepcopy(pipeline_config['pipeline_stages'][pipeline_stage]['stage_steps'][pipeline_step])\n",
    "    step_params_config = step_config['step_params']\n",
    "    \n",
    "    # use paths to fill path-depended params TODO: preprocess config to do this\n",
    "    step_params_config['software_params']['config_file'] = os.path.join(paths['config'],\n",
    "                                                                step_params_config['software_params']['config_file'])\n",
    "    step_params_config['software_params']['temp_path'] = paths[step_params_config['software_params']['temp_path']]\n",
    "    \n",
    "    # prepare io params   \n",
    "    # input catalogs\n",
    "    input_catalogs_format = to_ext_format(step_params_config['input_params']['catalogs_format'])\n",
    "    input_catalogs_dir = paths[step_params_config['input_params']['catalogs_dir']]\n",
    "    icfs = build_filenames_list(input_images_names, new_ext=input_catalogs_format, \n",
    "                                    new_path=input_catalogs_dir)\n",
    "    \n",
    "    # output psf models\n",
    "    output_psf_models_format = to_ext_format(step_params_config['output_params']['psf_models_format'])\n",
    "    output_psf_models_dir = paths[step_params_config['output_params']['psf_models_dir']]\n",
    "    # TODO: ispect BUG in PSFEx, it replaces PSF_SUFFIX only last extension part (after last .)\n",
    "    step_params_config['software_params']['config']['PSF_SUFFIX'] = ext_split(output_psf_models_format)[-1]\n",
    "    step_params_config['software_params']['config']['PSF_DIR'] = output_psf_models_dir\n",
    "    opmfs = build_filenames_list(input_images_names, new_ext=output_psf_models_format, \n",
    "                                    new_path=output_psf_models_dir)\n",
    "    \n",
    "    # output is list of calculated psf models full pathnames\n",
    "    #\n",
    "    \n",
    "    calculate_psf_separately = False\n",
    "    if 'calculate_psf_separately' in step_params_config['output_params'].keys():\n",
    "        if step_params_config['output_params']['calculate_psf_separately'] == 'true':\n",
    "            calculate_psf_separately = True\n",
    "            \n",
    "    software_params = step_params_config['software_params'] # TODO: deepcopy or not?\n",
    "    \n",
    "    if calculate_psf_separately:\n",
    "        for icf in icfs:\n",
    "            psfex = aw.api.Astromatic(**software_params)\n",
    "            #this_cmd, kwargs2 = psfex.build_cmd(icf, **software_params)\n",
    "            #print this_cmd\n",
    "            psfex.run(icf)\n",
    "    else:\n",
    "        psfex = aw.api.Astromatic(**software_params)\n",
    "        #this_cmd, kwargs2 = psfex.build_cmd(' '.join(icfs), **software_params)\n",
    "        #print this_cmd\n",
    "        psfex.run(' '.join(icfs)) # TODO: create file with catalogs list, and pass as @catalogs_list\n",
    "    \n",
    "    return opmfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h3>Test of pipeline</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input images are saved to temp folder, beause futher processing needs images and .head files in same directory.<br>\n",
    "Define list of input images, raw and tech frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GRB130427_R60_001_001',\n",
       " 'GRB130427_R60_001_002',\n",
       " 'GRB130427_R60_001_003',\n",
       " 'GRB130427_R60_001_004',\n",
       " 'GRB130427_R60_001_005',\n",
       " 'GRB130427_R60_001_006',\n",
       " 'GRB130427_R60_001_007',\n",
       " 'GRB130427_R60_001_008',\n",
       " 'GRB130427_R60_001_009',\n",
       " 'GRB130427_R60_001_010',\n",
       " 'GRB130427_R60_001_011',\n",
       " 'GRB130427_R60_001_012',\n",
       " 'GRB130427_R60_001_013',\n",
       " 'GRB130427_R60_001_014',\n",
       " 'GRB130427_R60_001_015',\n",
       " 'GRB130427_R60_001_016']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_images_names = sorted(get_fits_images_from_dir(paths['temp'], pipeline_config['input_images_format'], True, True))\n",
    "input_images_names = filter_fits_by_header(input_images_names, paths['temp'], EXPTYPE='Light')\n",
    "input_images_names = [change_file_extension(os.path.basename(iin), '') for iin in input_images_names]\n",
    "input_images_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Pipeline basic functions calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stage: images_calibration\n",
    "catalogs_1 = create_catalogs(input_images_names, paths, pipeline_config, 'images_calibration', 'create_calibration_catalogs')\n",
    "calculate_calibration(input_images_names, paths, pipeline_config, 'images_calibration', 'create_calibration_headers')\n",
    "swarped_images_1, _ = swarp_images(input_images_names, paths, pipeline_config, 'images_calibration', 'calibrate_images')\n",
    "catalogs_2 = create_catalogs(input_images_names, paths, pipeline_config, 'images_calibration', 'create_calibrated_images_catalogs')\n",
    "calculate_psf(input_images_names, paths, pipeline_config, 'images_calibration', 'model_psf_calibrated_images_catalogs')\n",
    "catalogs_3 = create_catalogs(input_images_names, paths, pipeline_config, 'images_calibration', 'create_calibrated_images_catalogs_psf')\n",
    "# stage: images_coaddition\n",
    "swarped_images_2, _ = swarp_images(input_images_names, paths, pipeline_config, 'images_coaddition', 'coadd_calibrated_images')\n",
    "catalogs_4 = create_catalogs(swarped_images_2, paths, pipeline_config, 'images_coaddition', 'create_coadded_image_catalog')\n",
    "calculate_psf(swarped_images_2, paths, pipeline_config, 'images_coaddition', 'model_psf_coadded_image_catalog')\n",
    "catalogs_5 = create_catalogs(swarped_images_2, paths, pipeline_config, 'images_coaddition', 'create_coadded_image_catalog_psf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_dict = {}\n",
    "cal_images = {im: fits_ImageHDU_from_file(im) for im in swarped_images_1}\n",
    "cal_cats = {cat: aw.utils.ldac.get_table_from_ldac(cat) for cat in catalogs_2}\n",
    "cal_cats_psf = {cat: aw.utils.ldac.get_table_from_ldac(cat) for cat in catalogs_3}\n",
    "output_dict['calibrated_images'] = cal_images\n",
    "output_dict['calibrated_images_catalogs'] = cal_cats\n",
    "output_dict['calibrated_images_catalogs_psf'] = cal_cats_psf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fits_ImageHDU_from_file(filename, mode='denywrite', lazy_load_hdus=False):\n",
    "    f = fits.open(filename, mode='denywrite', lazy_load_hdus=False)\n",
    "    fits_image = fits.ImageHDU(data=f[0].data, header=f[0].header)\n",
    "    f.close()\n",
    "    return fits_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_dict_2 = {}\n",
    "cal_images = {im: fits_ImageHDU_from_file(im) for im in swarped_images_2}\n",
    "cal_cats = {cat: aw.utils.ldac.get_table_from_ldac(cat) for cat in catalogs_4}\n",
    "cal_cats_psf = {cat: aw.utils.ldac.get_table_from_ldac(cat) for cat in catalogs_5}\n",
    "output_dict_2['calibrated_images'] = cal_images\n",
    "output_dict_2['calibrated_images_catalogs'] = cal_cats\n",
    "output_dict_2['calibrated_images_catalogs_psf'] = cal_cats_psf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(catalogs_4[0], 'rb') as fid:\n",
    "    data = BytesIO(fid.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calibrated_images_catalogs_psf 17\n",
      "calibrated_images 17\n",
      "calibrated_images_catalogs 17\n",
      "calibrated_images_catalogs_psf 16\n",
      "calibrated_images 16\n",
      "calibrated_images_catalogs 16\n"
     ]
    }
   ],
   "source": [
    "for key in res_dict.keys():\n",
    "    print key, len(res_dict[key].keys())\n",
    "for key in output_dict_4.keys():\n",
    "    print key, len(output_dict_4[key].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_dict_3 = copy.deepcopy(output_dict_2)\n",
    "output_dict_4 = copy.deepcopy(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# each input dict is a dict{type:dict{fn:bytes}}\n",
    "def ggg(dict1, dict2):\n",
    "    res = {}\n",
    "    for k, v in dict1.iteritems():\n",
    "        if k not in res.keys():\n",
    "            res[k] = {}\n",
    "        for kk, vv in dict1[k].iteritems():\n",
    "            res[k][kk] = vv\n",
    "\n",
    "    for k, v in dict2.iteritems():\n",
    "        if k not in res.keys():\n",
    "            res[k] = {}\n",
    "        for kk, vv in dict2[k].iteritems():\n",
    "            res[k][kk] = vv\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_dict = ggg(output_dict_3, output_dict_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = res_dict['calibrated_images'].items()\n",
    "image = f[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fits.writeto('test.fits', data=image.data, header=image.header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tbl_tuple = res_dict['calibrated_images_catalogs_psf'].items()[0]\n",
    "aw.utils.ldac.save_table_as_ldac(tbl_tuple[1], 'test_cat.cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Check coadded image catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "&lt;Table length=297&gt;\n",
       "<table id=\"table140138976354192\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>NUMBER</th><th>EXT_NUMBER</th><th>XWIN_IMAGE</th><th>YWIN_IMAGE</th><th>ERRAWIN_IMAGE</th><th>ERRBWIN_IMAGE</th><th>ERRTHETAWIN_IMAGE</th><th>XWIN_WORLD</th><th>YWIN_WORLD</th><th>FLUX_AUTO</th><th>FLUXERR_AUTO</th><th>FLAGS</th><th>FLAGS_WEIGHT</th><th>FLUX_RADIUS</th><th>ELONGATION</th><th>MAG_AUTO</th><th>MAGERR_AUTO</th><th>ALPHAPSF_SKY</th><th>DELTAPSF_SKY</th><th>ERRX2PSF_WORLD</th><th>ERRY2PSF_WORLD</th><th>FLUX_PSF</th><th>FLUXERR_PSF</th><th>MAG_PSF</th><th>MAGERR_PSF</th><th>FLUXERR_APER</th><th>FLUX_APER</th><th>VIGNET [20,20]</th><th>SNR_WIN</th></tr></thead>\n",
       "<thead><tr><th></th><th></th><th>pix</th><th>pix</th><th>pix</th><th>pix</th><th>deg</th><th>deg</th><th>deg</th><th>ct</th><th>ct</th><th></th><th></th><th>pix</th><th></th><th>mag</th><th>mag</th><th>deg</th><th>deg</th><th>deg2</th><th>deg2</th><th>ct</th><th>ct</th><th>mag</th><th>mag</th><th>ct</th><th>ct</th><th>ct</th><th></th></tr></thead>\n",
       "<thead><tr><th>int32</th><th>int16</th><th>float64</th><th>float64</th><th>float32</th><th>float32</th><th>float32</th><th>float64</th><th>float64</th><th>float32</th><th>float32</th><th>int16</th><th>int16</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th></tr></thead>\n",
       "<tr><td>1</td><td>1</td><td>199.010602485</td><td>37.9179525588</td><td>0.0102173</td><td>0.0101383</td><td>-72.7561</td><td>173.159134955</td><td>27.6451661357</td><td>1796.45</td><td>13.5765</td><td>4</td><td>0</td><td>2.69306</td><td>1.51522</td><td>-8.13603</td><td>0.00820735</td><td>173.159179092</td><td>27.6453762634</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>3.24317</td><td>830.273</td><td>0.611328 .. 0.419922</td><td>299.755</td></tr>\n",
       "<tr><td>2</td><td>1</td><td>384.74375879</td><td>11.4640961905</td><td>0.0801357</td><td>0.0793144</td><td>-59.5147</td><td>173.103225995</td><td>27.6381057041</td><td>109.568</td><td>5.70855</td><td>4</td><td>0</td><td>1.93767</td><td>1.17387</td><td>-5.09921</td><td>0.0565809</td><td>173.103194583</td><td>27.6381976196</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.77788</td><td>66.6823</td><td>-1.14258 .. 0.621094</td><td>29.7116</td></tr>\n",
       "<tr><td>3</td><td>1</td><td>166.723685832</td><td>12.7728482889</td><td>0.0465225</td><td>0.0454836</td><td>44.4767</td><td>173.16885299</td><td>27.6384595617</td><td>85.0137</td><td>4.074</td><td>4</td><td>0</td><td>1.27141</td><td>1.61323</td><td>-4.82372</td><td>0.052043</td><td>173.169006949</td><td>27.638351102</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.76513</td><td>56.0308</td><td>-16.8457 .. 0.699219</td><td>34.5657</td></tr>\n",
       "<tr><td>4</td><td>1</td><td>279.968759826</td><td>11.9314666831</td><td>0.0777</td><td>0.0776669</td><td>50.6996</td><td>173.134764638</td><td>27.6382365261</td><td>39.8438</td><td>2.49137</td><td>4</td><td>0</td><td>0.757805</td><td>1.01051</td><td>-4.0009</td><td>0.0679059</td><td>173.134907573</td><td>27.6381190897</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.74466</td><td>38.245</td><td>-32.5586 .. 0.960938</td><td>38.5539</td></tr>\n",
       "<tr><td>5</td><td>1</td><td>224.015763844</td><td>497.728014515</td><td>0.191113</td><td>0.190813</td><td>-17.0798</td><td>173.151613486</td><td>27.7677824754</td><td>195.299</td><td>9.30909</td><td>4</td><td>0</td><td>3.92954</td><td>1.50413</td><td>-5.72675</td><td>0.0517651</td><td>173.151550933</td><td>27.7678003745</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.76419</td><td>46.652</td><td>-1.09375 .. -0.162109</td><td>24.8686</td></tr>\n",
       "<tr><td>6</td><td>1</td><td>265.106121939</td><td>494.994342509</td><td>0.0258518</td><td>0.0251956</td><td>-89.6495</td><td>173.13923008</td><td>27.7670534141</td><td>44.8711</td><td>2.2562</td><td>4</td><td>0</td><td>0.750279</td><td>1.64554</td><td>-4.12992</td><td>0.0546062</td><td>173.139139444</td><td>27.7671156981</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.76045</td><td>45.1969</td><td>-0.142578 .. 1.99023</td><td>36.2554</td></tr>\n",
       "<tr><td>7</td><td>1</td><td>413.670294402</td><td>492.016041988</td><td>0.166282</td><td>0.165614</td><td>72.532</td><td>173.094457798</td><td>27.7662496947</td><td>93.8516</td><td>5.53493</td><td>4</td><td>0</td><td>2.93524</td><td>1.36369</td><td>-4.9311</td><td>0.0640473</td><td>173.094394051</td><td>27.7663293096</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.75319</td><td>46.6174</td><td>-0.5 .. 0.0273438</td><td>21.3961</td></tr>\n",
       "<tr><td>8</td><td>1</td><td>132.402703786</td><td>491.673258806</td><td>0.0961593</td><td>0.095979</td><td>-65.5327</td><td>173.17922254</td><td>27.766164091</td><td>107.361</td><td>6.48395</td><td>4</td><td>0</td><td>2.1245</td><td>1.27958</td><td>-5.07712</td><td>0.0655876</td><td>173.179357798</td><td>27.7661493979</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.77549</td><td>67.1825</td><td>0.078125 .. 0.160156</td><td>27.0353</td></tr>\n",
       "<tr><td>9</td><td>1</td><td>120.057925169</td><td>490.984825095</td><td>0.103238</td><td>0.102344</td><td>43.8775</td><td>173.182942784</td><td>27.7659795796</td><td>61.373</td><td>2.7247</td><td>4</td><td>0</td><td>1.88585</td><td>1.71186</td><td>-4.46994</td><td>0.0482137</td><td>173.182958466</td><td>27.7659810115</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.76302</td><td>52.9416</td><td>-1e+30 .. 0.417969</td><td>22.4731</td></tr>\n",
       "<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "<tr><td>288</td><td>1</td><td>32.0060470175</td><td>25.9898118316</td><td>0.0781977</td><td>0.0781924</td><td>34.4498</td><td>173.209406925</td><td>27.6419716347</td><td>48.2168</td><td>2.85151</td><td>4</td><td>0</td><td>0.407288</td><td>1.00792</td><td>-4.208</td><td>0.0642254</td><td>173.209407112</td><td>27.6419717671</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.75884</td><td>46.8463</td><td>-0.400391 .. 0.283203</td><td>61.8731</td></tr>\n",
       "<tr><td>289</td><td>1</td><td>375.254454999</td><td>46.6388456511</td><td>0.0421174</td><td>0.0409206</td><td>71.9113</td><td>173.106078939</td><td>27.6474864671</td><td>43.9453</td><td>2.97858</td><td>4</td><td>0</td><td>0.897482</td><td>1.00565</td><td>-4.10728</td><td>0.0736083</td><td>173.106006618</td><td>27.6474496852</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.74281</td><td>43.5503</td><td>-0.9375 .. -0.28125</td><td>29.4231</td></tr>\n",
       "<tr><td>290</td><td>1</td><td>328.013829942</td><td>21.638293622</td><td>0.0348716</td><td>0.0285189</td><td>-89.606</td><td>173.120301802</td><td>27.6408230536</td><td>41.375</td><td>2.84697</td><td>4</td><td>0</td><td>0.754194</td><td>1.00395</td><td>-4.04184</td><td>0.0747264</td><td>173.120305438</td><td>27.6409173324</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.75491</td><td>41.1312</td><td>-0.972656 .. -0.583984</td><td>32.6409</td></tr>\n",
       "<tr><td>291</td><td>1</td><td>293.706869104</td><td>23.0190707708</td><td>0.100642</td><td>0.100244</td><td>9.26512</td><td>173.130628854</td><td>27.6411928077</td><td>58.2363</td><td>4.06154</td><td>4</td><td>0</td><td>1.6572</td><td>1.73283</td><td>-4.41298</td><td>0.0757401</td><td>173.130896499</td><td>27.6410849056</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.76461</td><td>51.2627</td><td>-0.869141 .. -0.285156</td><td>20.3173</td></tr>\n",
       "<tr><td>292</td><td>1</td><td>216.294384947</td><td>18.7187587499</td><td>0.0370717</td><td>0.0353802</td><td>-40.122</td><td>173.153931616</td><td>27.6400467427</td><td>44.2598</td><td>2.17017</td><td>4</td><td>0</td><td>0.849838</td><td>1.01087</td><td>-4.11502</td><td>0.0532494</td><td>173.153869532</td><td>27.6399911475</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.74332</td><td>44.2889</td><td>-0.607422 .. 0.419922</td><td>32.1569</td></tr>\n",
       "<tr><td>293</td><td>1</td><td>452.717967176</td><td>15.9451429063</td><td>0.027974</td><td>0.0253722</td><td>-1.22847</td><td>173.082764179</td><td>27.6392928144</td><td>52.9824</td><td>3.04708</td><td>4</td><td>0</td><td>0.803818</td><td>1.58144</td><td>-4.31033</td><td>0.0624571</td><td>173.082678239</td><td>27.639218571</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.77205</td><td>54.1441</td><td>-0.210938 .. -0.291016</td><td>40.045</td></tr>\n",
       "<tr><td>294</td><td>1</td><td>405.544321338</td><td>15.7242192855</td><td>0.0941633</td><td>0.0927865</td><td>-24.3254</td><td>173.09696424</td><td>27.6392396559</td><td>58.4023</td><td>4.10794</td><td>4</td><td>0</td><td>1.43031</td><td>2.55834</td><td>-4.41608</td><td>0.0763878</td><td>173.096826468</td><td>27.6391805247</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.74828</td><td>39.494</td><td>0.242188 .. 8.09961</td><td>19.2782</td></tr>\n",
       "<tr><td>295</td><td>1</td><td>145.007262014</td><td>43.9996582843</td><td>0.0758782</td><td>0.0758741</td><td>73.0054</td><td>173.175392155</td><td>27.6467854823</td><td>46.2773</td><td>2.85023</td><td>4</td><td>0</td><td>0.412839</td><td>1.00392</td><td>-4.16342</td><td>0.0668868</td><td>173.175392389</td><td>27.6467856003</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.75325</td><td>46.7107</td><td>1.09375 .. -0.205078</td><td>58.9932</td></tr>\n",
       "<tr><td>296</td><td>1</td><td>170.869393596</td><td>499.065934783</td><td>0.0728355</td><td>0.057267</td><td>44.9155</td><td>173.167630344</td><td>27.7681377249</td><td>71.416</td><td>4.46682</td><td>7</td><td>0</td><td>0.847295</td><td>1.2832</td><td>-4.63449</td><td>0.0679255</td><td>173.167737914</td><td>27.7679909723</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.75929</td><td>59.5003</td><td>-0.503906 .. 6.11719</td><td>63.8977</td></tr>\n",
       "<tr><td>297</td><td>1</td><td>167.23057642</td><td>496.098094804</td><td>0.124972</td><td>0.124231</td><td>-3.79077</td><td>173.16872682</td><td>27.7673461304</td><td>156.957</td><td>7.49893</td><td>7</td><td>0</td><td>3.07326</td><td>1.40271</td><td>-5.48945</td><td>0.0518858</td><td>173.16859519</td><td>27.7674053964</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.0</td><td>99.0</td><td>2.76222</td><td>67.7699</td><td>0.792969 .. -0.617188</td><td>29.739</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Table length=297>\n",
       "NUMBER EXT_NUMBER   XWIN_IMAGE  ... FLUX_APER     VIGNET [20,20]     SNR_WIN\n",
       "                       pix      ...     ct              ct                  \n",
       "int32    int16       float64    ...  float32         float32         float32\n",
       "------ ---------- ------------- ... --------- ---------------------- -------\n",
       "     1          1 199.010602485 ...   830.273   0.611328 .. 0.419922 299.755\n",
       "     2          1  384.74375879 ...   66.6823   -1.14258 .. 0.621094 29.7116\n",
       "     3          1 166.723685832 ...   56.0308   -16.8457 .. 0.699219 34.5657\n",
       "     4          1 279.968759826 ...    38.245   -32.5586 .. 0.960938 38.5539\n",
       "     5          1 224.015763844 ...    46.652  -1.09375 .. -0.162109 24.8686\n",
       "     6          1 265.106121939 ...   45.1969   -0.142578 .. 1.99023 36.2554\n",
       "     7          1 413.670294402 ...   46.6174      -0.5 .. 0.0273438 21.3961\n",
       "     8          1 132.402703786 ...   67.1825   0.078125 .. 0.160156 27.0353\n",
       "     9          1 120.057925169 ...   52.9416     -1e+30 .. 0.417969 22.4731\n",
       "   ...        ...           ... ...       ...                    ...     ...\n",
       "   288          1 32.0060470175 ...   46.8463  -0.400391 .. 0.283203 61.8731\n",
       "   289          1 375.254454999 ...   43.5503    -0.9375 .. -0.28125 29.4231\n",
       "   290          1 328.013829942 ...   41.1312 -0.972656 .. -0.583984 32.6409\n",
       "   291          1 293.706869104 ...   51.2627 -0.869141 .. -0.285156 20.3173\n",
       "   292          1 216.294384947 ...   44.2889  -0.607422 .. 0.419922 32.1569\n",
       "   293          1 452.717967176 ...   54.1441 -0.210938 .. -0.291016  40.045\n",
       "   294          1 405.544321338 ...    39.494    0.242188 .. 8.09961 19.2782\n",
       "   295          1 145.007262014 ...   46.7107   1.09375 .. -0.205078 58.9932\n",
       "   296          1 170.869393596 ...   59.5003   -0.503906 .. 6.11719 63.8977\n",
       "   297          1  167.23057642 ...   67.7699  0.792969 .. -0.617188  29.739"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog = aw.utils.ldac.get_table_from_ldac('test_cat.cat')\n",
    "catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local hardcoded pipelines (testing astromatic software)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input: set of images, paths\n",
    "Output: coadded image, catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# by defualt, we search for images in paths['images']\n",
    "def pipeline_1(paths):\n",
    "    print \"prepare...\"\n",
    "    # prepare\n",
    "    fits_format = 'fit'\n",
    "    filenames = sorted(get_fits_images_from_dir(paths['images'], files_format=fits_format))\n",
    "    exposures = []\n",
    "    for filename in filenames:\n",
    "        exposures.append({'image': os.path.join(paths['images'], filename)})\n",
    "    # STEP 1: basic extraction with SExtractor (generate catalogs) for each image\n",
    "    catalog_names = []\n",
    "    print 'begin SExtractor execution'\n",
    "    for files in exposures:\n",
    "        print files['image'] + ':'\n",
    "        # Create names for the output catalogs for each image (in temp dir)\n",
    "        catalog_names.append(os.path.join(paths['temp'],\n",
    "            os.path.basename(files['image']).replace('.'+fits_format, '.cat')))\n",
    "        # set params for sextractor execution\n",
    "        sex_kwargs_1 = {'code': 'SExtractor'}\n",
    "        sex_kwargs_1['config_file'] = os.path.join(paths['config'], 'default.sex')\n",
    "        sex_kwargs_1['config'] = {'CATALOG_NAME': catalog_names[-1]}\n",
    "        sex_kwargs_1['config']['CATALOG_TYPE'] = 'FITS_LDAC'\n",
    "        sex_kwargs_1['config']['FILTER'] = 'N'\n",
    "        sex_kwargs_1['temp_path'] = paths['temp']\n",
    "        sex_kwargs_1['params'] = ['NUMBER', 'EXT_NUMBER', 'XWIN_IMAGE', 'YWIN_IMAGE', 'AWIN_IMAGE', 'BWIN_IMAGE',\n",
    "                                  'ERRAWIN_IMAGE','ERRBWIN_IMAGE', 'ERRTHETAWIN_IMAGE', 'ERRA_WORLD', 'ERRB_WORLD', \n",
    "                                  'ERRTHETA_WORLD', 'X_WORLD', 'Y_WORLD', 'XWIN_WORLD', 'YWIN_WORLD', \n",
    "                                  'FLUX_AUTO', 'FLUX_MAX', 'MAG_AUTO', 'FLUXERR_AUTO', \n",
    "                                  'FLAGS', 'FLUX_RADIUS', 'ELONGATION']\n",
    "        sextractor = aw.api.Astromatic(**sex_kwargs_1)\n",
    "        print sextractor.run(files['image'])\n",
    "    print 'all catalogs successfully created'\n",
    "    \n",
    "    # STEP 2: compute solution for extracted catalogs\n",
    "    print 'SCAMP now working...'\n",
    "    scamp_kwargs_1 = {\n",
    "        'config': {\n",
    "            'ASTREF_CATALOG': 'USNO-B1',\n",
    "            'ASTREF_BAND': 'DEFAULT',\n",
    "            'SOLVE_PHOTOM': 'N',\n",
    "            'CHECKPLOT_DEV': 'NULL'\n",
    "            }\n",
    "    }\n",
    "    scamp_kwargs_1['code'] = 'SCAMP'\n",
    "    scamp_kwargs_1['config_file'] = os.path.join(paths['config'], 'default.scamp')\n",
    "    scamp = aw.api.Astromatic(**scamp_kwargs_1)\n",
    "    this_cmd, kwargs2 = scamp.build_cmd(catalog_names, **scamp_kwargs_1)\n",
    "    #print this_cmd\n",
    "    print scamp.run(catalog_names)\n",
    "    \n",
    "    print 'SWarp now working...'\n",
    "    # STEP 3: use SWarp for coadd images using .head files, computed with SCAMP\n",
    "    coadd_fits_name = 'coadded.fits'\n",
    "    coadd_fits_filename = os.path.join(paths['temp'], coadd_fits_name)\n",
    "    swarp_kwargs_1 = {\n",
    "        'config': {\n",
    "            'IMAGEOUT_NAME': coadd_fits_filename,\n",
    "            'WEIGHTOUT_NAME': coadd_fits_filename.replace('.fits','.wtmap.fits'),\n",
    "        }\n",
    "        }\n",
    "    swarp_kwargs_1['code'] = 'SWarp'\n",
    "    swarp_kwargs_1['config_file'] = os.path.join(paths['config'], 'default.swarp')\n",
    "    swarp = aw.api.Astromatic(**swarp_kwargs_1)\n",
    "    this_cmd, kwargs2 = swarp.build_cmd([exp['image'] for exp in exposures], **swarp_kwargs_1)\n",
    "    #print this_cmd\n",
    "    print swarp.run([exp['image'] for exp in exposures])\n",
    "    #return\n",
    "    # STEP 4: extract object from coadded image using PSF\n",
    "    print 'SExtractor now working...'\n",
    "    coadd_weight_map = coadd_fits_filename.replace('.fits','.wtmap.fits')\n",
    "    sex_kwargs_2 = {\n",
    "            'config': {\n",
    "                'CATALOG_NAME': coadd_fits_filename.replace('.fits', '.cat'),\n",
    "                'CATALOG_TYPE': 'FITS_LDAC',\n",
    "                'FILTER': 'N',\n",
    "                'WEIGHT_IMAGE': coadd_weight_map,\n",
    "                'WEIGHT_TYPE': 'MAP_WEIGHT',        \n",
    "            },\n",
    "            'params': ['NUMBER', 'EXT_NUMBER', 'XWIN_IMAGE', 'YWIN_IMAGE', 'ERRAWIN_IMAGE',\n",
    "                'ERRBWIN_IMAGE', 'ERRTHETAWIN_IMAGE', 'XWIN_WORLD', 'YWIN_WORLD', 'FLUX_APER(1)',\n",
    "                'FLUXERR_APER(1)', 'FLAGS', 'FLAGS_WEIGHT', 'FLUX_RADIUS',\n",
    "                'ELONGATION', 'VIGNET(20,20)', 'SNR_WIN']\n",
    "        }\n",
    "    sex_kwargs_2['code'] = 'SExtractor'\n",
    "    sex_kwargs_2['config_file'] = os.path.join(paths['config'], 'default.sex')\n",
    "    sex_kwargs_2['temp_path'] = paths['temp']\n",
    "    sextractor = aw.api.Astromatic(**sex_kwargs_2)\n",
    "    print sextractor.run(coadd_fits_filename)\n",
    "    \n",
    "    # Calculate PSF\n",
    "    print 'PSFex now working...'\n",
    "    psf_kwargs_1 = {\n",
    "        'config': {\n",
    "            'PHOT_APERTURES':'15'\n",
    "            'CENTER_KEYS': 'XWIN_IMAGE,YWIN_IMAGE',\n",
    "            'PSFVAR_KEYS': 'XWIN_IMAGE,YWIN_IMAGE',\n",
    "            'PHOTFLUX_KEY': 'FLUX_AUTO',\n",
    "            'PHOTFLUXERR_KEY': 'FLUXERR_AUTO',\n",
    "            'CHECKPLOT_DEV': 'NULL',\n",
    "            'STABILITY_TYPE':  'EXPOSURE',\n",
    "            'PSF_SUFFIX': '.psf'\n",
    "        }\n",
    "    }\n",
    "    psf_kwargs_1['code'] = 'PSFEx'\n",
    "    psf_kwargs_1['config_file'] = os.path.join(paths['config'], 'default.psfex')\n",
    "    psf_kwargs_1['temp_path'] = paths['temp']\n",
    "    psfex = aw.api.Astromatic(**psf_kwargs_1)\n",
    "    print psfex.run(coadd_fits_filename.replace('.fits', '.cat'))\n",
    "    \n",
    "    # Calculate PSF photometry for coadded image\n",
    "    print 'SExtractor now working...'\n",
    "    output_cat_name = 'result.ldac.fits'\n",
    "    catalog_name = os.path.join(paths['catalogs'], output_cat_name)\n",
    "    sex_kwargs_3 = {\n",
    "            'config': {\n",
    "                'PSF_NAME': os.path.join(coadd_fits_filename.replace('.fits', '.psf')),\n",
    "                'CATALOG_TYPE': 'FITS_LDAC',\n",
    "                'FILTER': 'N',\n",
    "                'CATALOG_NAME': catalog_name,\n",
    "                'WEIGHT_IMAGE': coadd_weight_map,\n",
    "                'WEIGHT_TYPE': 'MAP_WEIGHT',\n",
    "            },\n",
    "            'params': ['NUMBER', 'EXT_NUMBER', 'XWIN_IMAGE', 'YWIN_IMAGE', 'ERRAWIN_IMAGE',\n",
    "                'ERRBWIN_IMAGE', 'ERRTHETAWIN_IMAGE', 'XWIN_WORLD', 'YWIN_WORLD', 'FLUX_AUTO',\n",
    "                'FLUXERR_AUTO', 'FLAGS', 'FLAGS_WEIGHT', 'FLUX_RADIUS',\n",
    "                'ELONGATION', 'MAG_AUTO', 'MAGERR_AUTO', 'ALPHAPSF_SKY', 'DELTAPSF_SKY',\n",
    "                'ERRX2PSF_WORLD','ERRY2PSF_WORLD', 'FLUX_PSF', 'FLUXERR_PSF', 'MAG_PSF', 'MAGERR_PSF']\n",
    "        }\n",
    "    sex_kwargs_3['code'] = 'SExtractor'\n",
    "    sex_kwargs_3['config_file'] = os.path.join(paths['config'], 'default.sex')\n",
    "    sex_kwargs_3['temp_path'] = paths['temp']\n",
    "    sextractor = aw.api.Astromatic(**sex_kwargs_3)\n",
    "    print sextractor.run(coadd_fits_filename)\n",
    "    return catalog_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare...\n",
      "begin SExtractor execution\n",
      "/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/images/GRB130427_R60_001_001.fit:\n",
      "{'status': 'success'}\n",
      "/home/ser/Dev/astro_engine/spark_pipeline/spark_pipeline_test_1/images/GRB130427_R60_001_002.fit:\n",
      "{'status': 'success'}\n",
      "all catalogs successfully created\n",
      "SCAMP now working...\n",
      "{'status': 'success'}\n",
      "SWarp now working...\n",
      "{'status': 'success'}\n"
     ]
    }
   ],
   "source": [
    "catalog_name = pipeline_1(paths)\n",
    "#catalog = aw.utils.ldac.get_table_from_ldac(catalog_name)\n",
    "#catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_pipeline(pipeline, exposures, ref_catalog='IKI', ref_band='DEFAULT',\n",
    "        stack_name = 'test_stack.fits', output_cat_name='test_psf.ldac.fits'):\n",
    "    # Generate catalogs from sextractor\n",
    "    catalog_names = []\n",
    "    for files in exposures:\n",
    "        # Create names for the output catalogs for each image\n",
    "        catalog_names.append(os.path.join(pipeline.paths['temp'],\n",
    "            os.path.basename(files['image']).replace('.fit', '.cat')))\n",
    "        kwargs = {\n",
    "            # image to SExtract\n",
    "            'files': files,\n",
    "            # Arguments to initialize Astromatic class\n",
    "            'api_kwargs': {\n",
    "                # Configuration parameters\n",
    "                'config': {\n",
    "                    'CATALOG_NAME': catalog_names[-1],\n",
    "                    'CATALOG_TYPE': 'FITS_LDAC',\n",
    "                    'FILTER': 'N',\n",
    "                    #'WEIGHT_TYPE': 'MAP_WEIGHT',\n",
    "                },\n",
    "                # Output parameters\n",
    "                'params': ['NUMBER', 'EXT_NUMBER', 'XWIN_IMAGE', 'YWIN_IMAGE', 'ERRAWIN_IMAGE',\n",
    "                    'ERRBWIN_IMAGE', 'ERRTHETAWIN_IMAGE', 'XWIN_WORLD', 'YWIN_WORLD', 'FLUX_AUTO',\n",
    "                    'FLUXERR_AUTO', 'IMAFLAGS_ISO', 'FLAGS', 'FLAGS_WEIGHT', 'FLUX_RADIUS',\n",
    "                    'ELONGATION'],\n",
    "            },\n",
    "        }\n",
    "        # Add the step to the pipeline\n",
    "        pipeline.add_step(aw.api.run_sex, ['step1', 'SExtractor'], **kwargs)\n",
    "\n",
    "    # Use SCAMP to get astrometric solutions\n",
    "    kwargs = {\n",
    "        'catalogs': catalog_names,\n",
    "        'api_kwargs': {\n",
    "            'config': {\n",
    "                'ASTREF_CATALOG': ref_catalog,\n",
    "                'ASTREF_BAND': ref_band,\n",
    "                'SOLVE_PHOTOM': 'N',\n",
    "                'CHECKPLOT_DEV': 'NULL'\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    pipeline.add_step(aw.api.run_scamp, ['step2', 'SCAMP'],**kwargs)\n",
    "\n",
    "    # Resample (rotate and scale) and combine (stack) images using SWarp\n",
    "    stack_filename = os.path.join(pipeline.paths['temp'], stack_name)\n",
    "    kwargs = {\n",
    "        'filenames': [exp['image'] for exp in exposures],\n",
    "        'api_kwargs': {\n",
    "            'config': {\n",
    "                'WEIGHT_TYPE': 'MAP_WEIGHT',\n",
    "                'WEIGHT_SUFFIX': '.wtmap.fits',\n",
    "                'IMAGEOUT_NAME': stack_filename,\n",
    "                'WEIGHTOUT_NAME': stack_filename.replace('.fits','.wtmap.fits'),\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    pipeline.add_step(aw.api.run_swarp, ['step3', 'SWarp'], **kwargs)\n",
    "\n",
    "    # Get positions in stack for PSF photometry (SExtractor -> PSFex -> SExtractor)\n",
    "    kwargs = {\n",
    "        'files': {\n",
    "            'image': stack_filename,\n",
    "            'wtmap': stack_filename.replace('.fits', '.wtmap.fits')\n",
    "        },\n",
    "        'api_kwargs': {\n",
    "            'config': {\n",
    "                'CATALOG_TYPE': 'FITS_LDAC',\n",
    "                'FILTER': 'N',\n",
    "                'WEIGHT_TYPE': 'MAP_WEIGHT',\n",
    "            },\n",
    "            'params': ['NUMBER', 'EXT_NUMBER', 'XWIN_IMAGE', 'YWIN_IMAGE', 'ERRAWIN_IMAGE',\n",
    "                'ERRBWIN_IMAGE', 'ERRTHETAWIN_IMAGE', 'XWIN_WORLD', 'YWIN_WORLD', 'FLUX_APER(1)',\n",
    "                'FLUXERR_APER(1)', 'FLAGS', 'FLAGS_WEIGHT', 'FLUX_RADIUS',\n",
    "                'ELONGATION', 'VIGNET(20,20)', 'SNR_WIN'],\n",
    "        }\n",
    "    }\n",
    "    pipeline.add_step(aw.api.run_sex, ['step4', 'SExtractor'], **kwargs)\n",
    "\n",
    "    # Calculate PSF\n",
    "    kwargs = {\n",
    "        'catalogs': stack_filename.replace('.fits', '.cat'),\n",
    "        'api_kwargs': {\n",
    "            'config': {\n",
    "                'CENTER_KEYS': 'XWIN_IMAGE,YWIN_IMAGE',\n",
    "                'PSFVAR_KEYS': 'XWIN_IMAGE,YWIN_IMAGE',\n",
    "                'CHECKPLOT_DEV': 'NULL',\n",
    "                'PSF_SUFFIX': '.psf'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    pipeline.add_step(aw.api.run_psfex, ['step5', 'PSFEx'], **kwargs)\n",
    "\n",
    "    # Calculate PSF photometry for coadded image\n",
    "    catalog_name = os.path.join(pipeline.paths['catalogs'], output_cat_name)\n",
    "    kwargs = {\n",
    "        'files': {\n",
    "            'image': stack_filename,\n",
    "            'wtmap': stack_filename.replace('.fits', '.wtmap.fits')\n",
    "        },\n",
    "        'api_kwargs': {\n",
    "            'config': {\n",
    "                'PSF_NAME': os.path.join(stack_filename.replace('.fits', '.psf')),\n",
    "                'CATALOG_TYPE': 'FITS_LDAC',\n",
    "                'FILTER': 'N',\n",
    "                'CATALOG_NAME': catalog_name,\n",
    "                'WEIGHT_TYPE': 'MAP_WEIGHT',\n",
    "            },\n",
    "            'params': ['NUMBER', 'EXT_NUMBER', 'XWIN_IMAGE', 'YWIN_IMAGE', 'ERRAWIN_IMAGE',\n",
    "                'ERRBWIN_IMAGE', 'ERRTHETAWIN_IMAGE', 'XWIN_WORLD', 'YWIN_WORLD', 'FLUX_AUTO',\n",
    "                'FLUXERR_AUTO', 'FLAGS', 'FLAGS_WEIGHT', 'FLUX_RADIUS',\n",
    "                'ELONGATION', 'MAG_AUTO', 'MAGERR_AUTO', 'ALPHAPSF_SKY', 'DELTAPSF_SKY',\n",
    "                'ERRX2PSF_WORLD','ERRY2PSF_WORLD', 'FLUX_PSF', 'FLUXERR_PSF', 'MAG_PSF', 'MAGERR_PSF'],\n",
    "        }\n",
    "    }\n",
    "    pipeline.add_step(aw.api.run_sex, ['step6', 'SExtractor'], **kwargs)\n",
    "\n",
    "    def save_output(pipeline, old_stack, new_stack, old_cat, new_cat):\n",
    "        # Copy the final stack and catalog from the temp folder\n",
    "        import shutil\n",
    "        # Move the weight map if it exists\n",
    "        if os.path.isfile(old_stack.replace('.fits', '.wtmap.fits')):\n",
    "            shutil.move(old_stack.replace('.fits', '.wtmap.fits'), new_stack.replace('.fits', '.wtmap.fits'))\n",
    "        shutil.move(old_stack, new_stack)\n",
    "        shutil.move(old_cat, new_cat)\n",
    "        result = {\n",
    "            'status': 'success'\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    kwargs = {\n",
    "        'old_stack': stack_filename,\n",
    "        'new_stack': os.path.join(pipeline.paths['stacks'], stack_name),\n",
    "        'old_cat': catalog_name,\n",
    "        'new_cat': os.path.join(pipeline.paths['catalogs'], output_cat_name)\n",
    "    }\n",
    "    pipeline.add_step(save_output, ['step7', 'save_output'], **kwargs)\n",
    "\n",
    "    return pipeline"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:iraf27]",
   "language": "python",
   "name": "conda-env-iraf27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
